{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import zscore\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# %% Ensure folder existence\n",
    "def ensure_folder_exists(folder_path):\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "folder_name = '../data/preprocessed_phishing'\n",
    "\n",
    "ensure_folder_exists(folder_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1749311 entries, 0 to 2499998\n",
      "Data columns (total 18 columns):\n",
      " #   Column              Dtype  \n",
      "---  ------              -----  \n",
      " 0   url                 object \n",
      " 1   source              object \n",
      " 2   label               object \n",
      " 3   url_length          int64  \n",
      " 4   starts_with_ip      bool   \n",
      " 5   url_entropy         float64\n",
      " 6   has_punycode        bool   \n",
      " 7   digit_letter_ratio  float64\n",
      " 8   dot_count           int64  \n",
      " 9   at_count            int64  \n",
      " 10  dash_count          int64  \n",
      " 11  tld_count           int64  \n",
      " 12  domain_has_digits   bool   \n",
      " 13  subdomain_count     int64  \n",
      " 14  nan_char_entropy    float64\n",
      " 15  has_internal_links  bool   \n",
      " 16  whois_data          object \n",
      " 17  domain_age_days     float64\n",
      "dtypes: bool(4), float64(4), int64(6), object(4)\n",
      "memory usage: 206.9+ MB\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "data_path = '../data/raw_data/phishing/out.csv'  # adjust the path as necessary\n",
    "df = pd.read_csv(data_path)\n",
    "df.head()\n",
    "\n",
    "# drop null values\n",
    "df_clean = df.dropna()\n",
    "\n",
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "legitimate    1096403\n",
      "phishing       652908\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "phishing      50000\n",
      "legitimate    50000\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "phishing      5000\n",
      "legitimate    5000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_clean['label'].value_counts())\n",
    "\n",
    "# Using a subset\n",
    "target_size = 50000\n",
    "target_size1 = 5000 #for hierarchical clustering\n",
    "\n",
    "legitimate_sample = df_clean[df_clean['label'] == 'legitimate'].sample(n=target_size, random_state=42)\n",
    "phishing_sample = df_clean[df_clean['label'] == 'phishing'].sample(n=target_size, random_state=42)\n",
    "legitimate_sample1 = df_clean[df_clean['label'] == 'legitimate'].sample(n=target_size1, random_state=42)\n",
    "phishing_sample1 = df_clean[df_clean['label'] == 'phishing'].sample(n=target_size1, random_state=42)\n",
    "\n",
    "\n",
    "# Combining the samples\n",
    "balanced_df = pd.concat([legitimate_sample, phishing_sample])\n",
    "small_balanced_df = pd.concat([legitimate_sample1, phishing_sample1])\n",
    "\n",
    "# Shuffling\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "small_balanced_df = small_balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(balanced_df['label'].value_counts())\n",
    "print(small_balanced_df['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting relevant columns\n",
    "numerical_features = ['url_length', 'url_entropy', 'digit_letter_ratio',\n",
    "                     'dot_count', 'at_count', 'dash_count', 'tld_count',\n",
    "                     'subdomain_count', 'nan_char_entropy', 'domain_age_days']\n",
    "# Add boolean features to the list of numerical features\n",
    "boolean_features = ['starts_with_ip', 'has_punycode', 'domain_has_digits', 'has_internal_links']\n",
    "categorical_features = ['url', 'source', 'whois_data']\n",
    "\n",
    "# Combine all features\n",
    "all_features = numerical_features + boolean_features + categorical_features\n",
    "target_features = ['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the balanced_df subset to a new CSV file\n",
    "file_path = os.path.join(folder_name, 'subset.csv')\n",
    "balanced_df.to_csv(file_path, index=False)\n",
    "\n",
    "file_path1 = os.path.join(folder_name, 'smaller_subset.csv')\n",
    "small_balanced_df.to_csv(file_path1, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Below we divide the data into test and train sets for supervisedd learning pre-processings**\n",
    "- Clustering does not require this because it is an unsupervised learning where labels arent required.\n",
    "\n",
    "**Scaling required for**\n",
    "- Clustering\n",
    "- Perceptron\n",
    "- K-Nearest Neighbors\t\n",
    "- Multi-Layer Perceptron\n",
    "\n",
    "**PCA required for**\n",
    "- Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set distribution:\n",
      "label\n",
      "legitimate    40000\n",
      "phishing      40000\n",
      "Name: count, dtype: int64\n",
      "Testing set distribution:\n",
      "label\n",
      "phishing      10000\n",
      "legitimate    10000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# %% Train-Test Split\n",
    "# Split the balanced dataset into train (80%) and test (20%)\n",
    "train_df, test_df = train_test_split(balanced_df, test_size=0.2, random_state=42, stratify=balanced_df['label'])\n",
    "\n",
    "# Save train and test datasets\n",
    "train_df.to_csv(os.path.join(folder_name, 'train.csv'), index=False)\n",
    "test_df.to_csv(os.path.join(folder_name, 'test.csv'), index=False)\n",
    "\n",
    "# Print class distributions\n",
    "print(f\"Training set distribution:\\n{train_df['label'].value_counts()}\")\n",
    "print(f\"Testing set distribution:\\n{test_df['label'].value_counts()}\")\n",
    "\n",
    "#for small\n",
    "train_df1, test_df1 = train_test_split(small_balanced_df, test_size=0.2, random_state=42, stratify=small_balanced_df['label'])\n",
    "train_df1.to_csv(os.path.join(folder_name, 'train_small.csv'), index=False)\n",
    "test_df1.to_csv(os.path.join(folder_name, 'test_small.csv'), index=False)\n",
    "# print(f\"Training set distribution:\\n{train_df1['label'].value_counts()}\")\n",
    "# print(f\"Testing set distribution:\\n{test_df1['label'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating 3 dataframes (full data, train, test)\n",
    "# all_df = balanced_df[numerical_features + target_features]\n",
    "all_df = balanced_df[all_features + target_features].copy()\n",
    "train_df = train_df[all_features + target_features].copy()\n",
    "test_df = test_df[all_features + target_features].copy()\n",
    "\n",
    "# for small\n",
    "all_df1 = small_balanced_df[all_features + target_features].copy()\n",
    "train_df1 = train_df1[all_features + target_features].copy()\n",
    "test_df1 = test_df1[all_features + target_features].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Label Encoding for simplicity in this example\n",
    "for cat_feature in categorical_features:\n",
    "    label_encoder = LabelEncoder()\n",
    "    all_df[cat_feature] = label_encoder.fit_transform(all_df[cat_feature])\n",
    "    train_df[cat_feature] = label_encoder.transform(train_df[cat_feature])\n",
    "    test_df[cat_feature] = label_encoder.transform(test_df[cat_feature])\n",
    "\n",
    "    # For small subset\n",
    "    all_df1[cat_feature] = label_encoder.transform(all_df1[cat_feature])\n",
    "    train_df1[cat_feature] = label_encoder.transform(train_df1[cat_feature])\n",
    "    test_df1[cat_feature] = label_encoder.transform(test_df1[cat_feature])\n",
    "\n",
    "# Encode target labels\n",
    "all_df['label_encoded'] = label_encoder.fit_transform(all_df['label'])\n",
    "train_df['label_encoded'] = label_encoder.transform(train_df['label'])\n",
    "test_df['label_encoded'] = label_encoder.transform(test_df['label'])\n",
    "\n",
    "# For small subset\n",
    "all_df1['label_encoded'] = label_encoder.transform(all_df1['label'])\n",
    "train_df1['label_encoded'] = label_encoder.transform(train_df1['label'])\n",
    "test_df1['label_encoded'] = label_encoder.transform(test_df1['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "1. Remove highly correlated features.\n",
    "2. Remove outliers using z-scores.\n",
    "3. Scale the data.\n",
    "4. Perform PCA for dimensionality reduction.\n",
    "5. Visualize the PCA-transformed data.\n",
    "6. Apply and visualize clustering results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 14 columns):\n",
      " #   Column              Non-Null Count   Dtype  \n",
      "---  ------              --------------   -----  \n",
      " 0   url_length          100000 non-null  int64  \n",
      " 1   url_entropy         100000 non-null  float64\n",
      " 2   digit_letter_ratio  100000 non-null  float64\n",
      " 3   dot_count           100000 non-null  int64  \n",
      " 4   at_count            100000 non-null  int64  \n",
      " 5   dash_count          100000 non-null  int64  \n",
      " 6   tld_count           100000 non-null  int64  \n",
      " 7   subdomain_count     100000 non-null  int64  \n",
      " 8   nan_char_entropy    100000 non-null  float64\n",
      " 9   domain_age_days     100000 non-null  float64\n",
      " 10  starts_with_ip      100000 non-null  bool   \n",
      " 11  has_punycode        100000 non-null  bool   \n",
      " 12  domain_has_digits   100000 non-null  bool   \n",
      " 13  has_internal_links  100000 non-null  bool   \n",
      "dtypes: bool(4), float64(4), int64(6)\n",
      "memory usage: 8.0 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing for clustering\n",
    "# clustering_df = all_df.drop(columns=['label', 'label_encoded'] + categorical_features)\n",
    "clustering_df = all_df.drop(columns=['label', 'label_encoded'] + categorical_features)\n",
    "clustering_df_with_label = all_df.drop(columns=categorical_features)\n",
    "# Convert boolean features to numeric if needed\n",
    "# clustering_df = clustering_df.astype(float)\n",
    "\n",
    "print(clustering_df.info())\n",
    "\n",
    "#for small\n",
    "clustering_df1 = all_df1.drop(columns=['label', 'label_encoded'] + categorical_features)\n",
    "clustering_df1_with_label = all_df.drop(columns=categorical_features)\n",
    "# print(clustering_df1.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove highly correlated features.\n",
    "correlation_matrix = clustering_df.corr()\n",
    "upper_tri = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column].abs() > 0.9)]\n",
    "clustering_df = clustering_df.drop(columns=to_drop)\n",
    "\n",
    "#for small\n",
    "correlation_matrix1 = clustering_df1.corr()\n",
    "upper_tri1 = correlation_matrix1.where(np.triu(np.ones(correlation_matrix1.shape), k=1).astype(bool))\n",
    "to_drop1 = [column for column in upper_tri1.columns if any(upper_tri1[column].abs() > 0.9)]\n",
    "clustering_df1 = clustering_df1.drop(columns=to_drop1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove outliers using z-scores.\n",
    "# # Convert boolean features to numeric if needed\n",
    "# clustering_df = clustering_df.astype(float)\n",
    "# clustering_df = clustering_df[(np.abs(zscore(clustering_df)) < 3).all(axis=1)]\n",
    "# print(clustering_df.info())\n",
    "\n",
    "# #for small\n",
    "# clustering_df1 = clustering_df1.astype(float)\n",
    "# clustering_df1 = clustering_df1[(np.abs(zscore(clustering_df1)) < 3).all(axis=1)]\n",
    "# # print(clustering_df1.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "clustering_df_scaled = scaler.fit_transform(clustering_df)\n",
    "\n",
    "#for small\n",
    "clustering_df_scaled1 = scaler.fit_transform(clustering_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components to retain 90% variance: 10\n",
      "Explained variance with 10 components: [0.22395213 0.34208606 0.44711621 0.53607682 0.62024986 0.69198608\n",
      " 0.75518846 0.81315588 0.85979079 0.90341478]\n"
     ]
    }
   ],
   "source": [
    "# Perform PCA without specifying the number of components\n",
    "pca = PCA()\n",
    "pca_transformed = pca.fit_transform(clustering_df_scaled)\n",
    "\n",
    "# Calculate cumulative variance explained by each component\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Find the number of components that retain at least 90% variance\n",
    "n_components_90 = np.argmax(cumulative_variance >= 0.9) + 1\n",
    "print(f\"Number of components to retain 90% variance: {n_components_90}\")\n",
    "\n",
    "# Apply PCA with the required number of components\n",
    "pca = PCA(n_components=n_components_90)\n",
    "pca.fit_transform(clustering_df_scaled)\n",
    "\n",
    "# Verify the explained variance\n",
    "print(f\"Explained variance with {n_components_90} components: {np.cumsum(pca.explained_variance_ratio_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Components needed to display atleast 90% data would be 10, but we shall use 2d clustering, hence our PCA shall be with 2 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "clustering_ready_df = pca.fit_transform(clustering_df_scaled)\n",
    "\n",
    "#for small\n",
    "clustering_ready_df1 = pca.fit_transform(clustering_df_scaled1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after preprocessing: (100000, 2)\n",
      "Shape after preprocessing small: (10000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Final clustering-ready DataFrame\n",
    "print(f\"Shape after preprocessing: {clustering_ready_df.shape}\")\n",
    "\n",
    "#for small\n",
    "print(f\"Shape after preprocessing small: {clustering_ready_df1.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = folder_name + '/clustering/'\n",
    "ensure_folder_exists(folder)\n",
    "\n",
    "# Save the clustering-ready DataFrame with PCA-reduced data\n",
    "clustering_ready_df = pd.DataFrame(clustering_ready_df, columns=['PCA1', 'PCA2'])  # Ensure column names are present\n",
    "clustering_ready_df.to_csv(os.path.join(folder,'clustering.csv'), index=False)\n",
    "\n",
    "#for small\n",
    "clustering_ready_df1 = pd.DataFrame(clustering_ready_df1, columns=['PCA1', 'PCA2'])  # Ensure column names are present\n",
    "clustering_ready_df1.to_csv(os.path.join(folder,'clustering_small.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "\n",
    "1. Decision Trees are not sensitive to feature scaling or outliers.\n",
    "2. Categorical labels must be encoded.\n",
    "3. No need for dimensionality reduction (like PCA).\n",
    "4. Handles missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing completed! Files saved as dt_X_train.csv, dt_X_test.csv, dt_y_train.csv, and dt_y_test.csv.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1: Load the dataset\n",
    "df_dt = balanced_df.copy()\n",
    "\n",
    "# Step 2: Drop irrelevant columns (e.g., 'url', 'whois_data')\n",
    "columns_to_drop = [\"url\", \"whois_data\", \"source\"]  # Adjust as necessary\n",
    "df_dt = df_dt.drop(columns=columns_to_drop, errors=\"ignore\")\n",
    "\n",
    "# Step 3: Handle missing values\n",
    "df_dt = df_dt.dropna()  # Drop rows with missing values\n",
    "\n",
    "# Step 4: Encode target labels\n",
    "label_encoder_dt = LabelEncoder()\n",
    "df_dt[\"label_encoded\"] = label_encoder_dt.fit_transform(df_dt[\"label\"])  # Binary encoding\n",
    "\n",
    "# Step 5: Convert Boolean columns to numeric\n",
    "boolean_columns_dt = df_dt.select_dtypes(include=[\"bool\"]).columns\n",
    "df_dt[boolean_columns_dt] = df_dt[boolean_columns_dt].astype(int)\n",
    "\n",
    "# Step 6: Separate features and target\n",
    "X_dt = df_dt.drop(columns=[\"label\", \"label_encoded\"])  # Features\n",
    "y_dt = df_dt[\"label_encoded\"]  # Target\n",
    "\n",
    "# Step 7: Train-test split\n",
    "X_train_dt, X_test_dt, y_train_dt, y_test_dt = train_test_split(X_dt, y_dt, test_size=0.2, random_state=42)\n",
    "\n",
    "folder = folder_name + '/decision_tree/'\n",
    "ensure_folder_exists(folder)\n",
    "\n",
    "# Step 8: Save preprocessed data\n",
    "X_train_dt.to_csv(os.path.join(folder, 'dt_X_train.csv'), index=False)\n",
    "X_test_dt.to_csv(os.path.join(folder, 'dt_X_test.csv'), index=False)\n",
    "y_train_dt.to_csv(os.path.join(folder, 'dt_y_train.csv'), index=False)\n",
    "y_test_dt.to_csv(os.path.join(folder, 'dt_y_test.csv'), index=False)\n",
    "\n",
    "print(\"Preprocessing completed! Files saved as dt_X_train.csv, dt_X_test.csv, dt_y_train.csv, and dt_y_test.csv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (80000, 17), Shape of X_test: (20000, 17)\n",
      "Sample of y_train:\n",
      "9567     0\n",
      "93297    0\n",
      "31947    0\n",
      "11663    1\n",
      "33026    0\n",
      "Name: label_encoded, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check processed data\n",
    "print(f\"Shape of X_train: {X_train.shape}, Shape of X_test: {X_test.shape}\")\n",
    "print(f\"Sample of y_train:\\n{y_train.head()}\")\n",
    "\n",
    "folder = folder_name + '/decision_tree/'\n",
    "ensure_folder_exists(folder)\n",
    "\n",
    "# Save preprocessed Decision Tree data\n",
    "X_train.to_csv(os.path.join(folder, 'dt_X_train.csv'), index=False)\n",
    "X_test.to_csv(os.path.join(folder, 'dt_X_test.csv'), index=False)\n",
    "y_train.to_csv(os.path.join(folder, 'dt_y_train.csv'), index=False)\n",
    "y_test.to_csv(os.path.join(folder, 'dt_y_test.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "1. Handle missing values\n",
    "2. No strict need for feature scaling, but it can improve numerical stability\n",
    "3. No need for dimensionality reduction (like PCA).\n",
    "4. Ensure labels and all features are numerical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and testing data for Naive Bayes\n",
    "nb_features = numerical_features  # Already defined earlier\n",
    "X_train_nb = train_df[nb_features].copy()\n",
    "y_train_nb = train_df['label_encoded'].copy()\n",
    "\n",
    "X_test_nb = test_df[nb_features].copy()\n",
    "y_test_nb = test_df['label_encoded'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_nb_scaled: (80000, 10), Shape of X_test_nb_scaled: (20000, 10)\n",
      "Sample of y_train_nb:\n",
      "9567     0\n",
      "93297    0\n",
      "31947    0\n",
      "11663    1\n",
      "33026    0\n",
      "Name: label_encoded, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Scale features (optional, recommended for Gaussian Naive Bayes)\n",
    "X_train_nb_scaled = pd.DataFrame(scaler.fit_transform(X_train_nb), columns=nb_features)\n",
    "X_test_nb_scaled = pd.DataFrame(scaler.transform(X_test_nb), columns=nb_features)\n",
    "\n",
    "# Check processed data\n",
    "print(f\"Shape of X_train_nb_scaled: {X_train_nb_scaled.shape}, Shape of X_test_nb_scaled: {X_test_nb_scaled.shape}\")\n",
    "print(f\"Sample of y_train_nb:\\n{y_train_nb.head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessed Naive Bayes data\n",
    "folder = folder_name + '/naive_bayes/'\n",
    "ensure_folder_exists(folder)\n",
    "\n",
    "X_train_nb_scaled.to_csv(os.path.join(folder, 'nb_X_train.csv'), index=False)\n",
    "X_test_nb_scaled.to_csv(os.path.join(folder, 'nb_X_test.csv'), index=False)\n",
    "y_train_nb.to_csv(os.path.join(folder, 'nb_y_train.csv'), index=False)\n",
    "y_test_nb.to_csv(os.path.join(folder, 'nb_y_test.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptrons & Multi-Layer Perceptron (MLP)\n",
    "1. Feature Scaling is Mandatory: MLP relies on gradient descent optimization, which performs better when the features are normalized.\n",
    "2. Label Encoding is Required: The target variable must be numerical. Use LabelEncoder for this purpose.\n",
    "3. No Specific Need for Dimensionality Reduction: However, if the dataset has very high dimensions, Principal Component Analysis (PCA) can be used for preprocessing.\n",
    "4. Handles Nonlinear Relationships: MLP can capture complex, nonlinear relationships between features.\n",
    "5. Sensitive to Hyper-parameters: Key parameters like learning rate, number of layers, and number of neurons need tuning for optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in 'label' for subset:\n",
      "['phishing' 'legitimate']\n",
      "Unique values in 'label_encoded' for balanced_df_perceptron:\n",
      "[1 0]\n",
      "Unique values in 'label_encoded' for small_balanced_df_perceptron:\n",
      "[1 0]\n",
      "Final unique values in 'label_encoded' for balanced_df_perceptron:\n",
      "[1 0]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 15 columns):\n",
      " #   Column              Non-Null Count   Dtype  \n",
      "---  ------              --------------   -----  \n",
      " 0   url_length          100000 non-null  float64\n",
      " 1   starts_with_ip      100000 non-null  bool   \n",
      " 2   url_entropy         100000 non-null  float64\n",
      " 3   has_punycode        100000 non-null  bool   \n",
      " 4   digit_letter_ratio  100000 non-null  float64\n",
      " 5   dot_count           100000 non-null  float64\n",
      " 6   at_count            100000 non-null  float64\n",
      " 7   dash_count          100000 non-null  float64\n",
      " 8   tld_count           100000 non-null  float64\n",
      " 9   domain_has_digits   100000 non-null  bool   \n",
      " 10  subdomain_count     100000 non-null  float64\n",
      " 11  nan_char_entropy    100000 non-null  float64\n",
      " 12  has_internal_links  100000 non-null  bool   \n",
      " 13  domain_age_days     100000 non-null  float64\n",
      " 14  label_encoded       100000 non-null  int64  \n",
      "dtypes: bool(4), float64(10), int64(1)\n",
      "memory usage: 8.8 MB\n",
      "Final unique values in 'label_encoded' for small_balanced_df_perceptron:\n",
      "[1 0]\n",
      "Preprocessed full data saved to: ../data/preprocessed_phishing/perceptron/perceptron.csv\n",
      "Preprocessed small data saved to: ../data/preprocessed_phishing/perceptron/small_perceptron.csv\n",
      "Unique values in 'label_encoded' after reloading from CSV:\n",
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique values in 'label' for subset:\")\n",
    "print(balanced_df['label'].unique())\n",
    "\n",
    "folder = folder_name + '/perceptron/'\n",
    "ensure_folder_exists(folder)\n",
    "\n",
    "# Copy dataset for MLP preprocessing\n",
    "balanced_df_perceptron = balanced_df.copy()\n",
    "small_balanced_df_perceptron = small_balanced_df.copy()  # For potential smaller dataset preprocessing\n",
    "\n",
    "# Drop unnecessary columns  \n",
    "balanced_df_perceptron = balanced_df_perceptron.drop(columns=categorical_features)\n",
    "small_balanced_df_perceptron = small_balanced_df_perceptron.drop(columns=categorical_features)\n",
    "\n",
    "# Encode the target column\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(['phishing', 'legitimate'])  # Define a consistent order for labels\n",
    "\n",
    "# Encode labels for each dataset\n",
    "balanced_df_perceptron['label_encoded'] = label_encoder.transform(balanced_df_perceptron['label'])\n",
    "small_balanced_df_perceptron['label_encoded'] = label_encoder.transform(small_balanced_df_perceptron['label'])\n",
    "\n",
    "# Drop the original label column\n",
    "balanced_df_perceptron = balanced_df_perceptron.drop(columns=['label'])\n",
    "small_balanced_df_perceptron = small_balanced_df_perceptron.drop(columns=['label'])\n",
    "\n",
    "# Verify the unique values after encoding\n",
    "print(\"Unique values in 'label_encoded' for balanced_df_perceptron:\")\n",
    "print(balanced_df_perceptron['label_encoded'].unique())\n",
    "\n",
    "print(\"Unique values in 'label_encoded' for small_balanced_df_perceptron:\")\n",
    "print(small_balanced_df_perceptron['label_encoded'].unique())\n",
    "\n",
    "# Scale numerical features\n",
    "\n",
    "# Separate numerical columns, excluding 'label_encoded'\n",
    "numerical_columns = [col for col in balanced_df_perceptron.select_dtypes(include=['int64', 'float64']).columns if col != 'label_encoded']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "balanced_df_perceptron[numerical_columns] = scaler.fit_transform(balanced_df_perceptron[numerical_columns])\n",
    "small_balanced_df_perceptron[numerical_columns] = scaler.transform(small_balanced_df_perceptron[numerical_columns])\n",
    "\n",
    "# Verify the 'label_encoded' column before saving\n",
    "print(\"Final unique values in 'label_encoded' for balanced_df_perceptron:\")\n",
    "print(balanced_df_perceptron['label_encoded'].unique())\n",
    "balanced_df_perceptron.info()\n",
    "\n",
    "print(\"Final unique values in 'label_encoded' for small_balanced_df_perceptron:\")\n",
    "print(small_balanced_df_perceptron['label_encoded'].unique())\n",
    "\n",
    "# # Save the processed datasets for later use\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "folder = folder_name + '/perceptron/'\n",
    "ensure_folder_exists(folder)\n",
    "# Save full dataset\n",
    "output_file = os.path.join(folder, 'perceptron.csv')\n",
    "balanced_df_perceptron.to_csv(output_file, index=False)\n",
    "\n",
    "# Save small dataset\n",
    "output_file_small = os.path.join(folder, 'small_perceptron.csv')\n",
    "small_balanced_df_perceptron.to_csv(output_file_small, index=False)\n",
    "\n",
    "print(f\"Preprocessed full data saved to: {output_file}\")\n",
    "print(f\"Preprocessed small data saved to: {output_file_small}\")\n",
    "\n",
    "# Reload the saved file to verify its contents\n",
    "small_balanced_df = pd.read_csv(output_file_small)\n",
    "print(\"Unique values in 'label_encoded' after reloading from CSV:\")\n",
    "print(small_balanced_df['label_encoded'].unique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbours\n",
    "1. Use numerical and boolean features only; drop categorical features.\n",
    "2. Encode boolean features as 0 and 1.\n",
    "3. Encode the target labels (phishing and legitimate) using LabelEncoder.\n",
    "4. Apply standard scaling (StandardScaler) to ensure all features are on a similar scale.\n",
    "5. Split the dataset into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN datasets prepared and saved to: ../data/preprocessed_phishing/knn/\n",
      "X_train shape: (80000, 14), y_train shape: (80000,)\n",
      "X_test shape: (20000, 14), y_test shape: (20000,)\n"
     ]
    }
   ],
   "source": [
    "# Define output folder for KNN\n",
    "folder = folder_name + '/knn/'\n",
    "ensure_folder_exists(folder)\n",
    "\n",
    "balanced_df_knn = balanced_df.copy()\n",
    "\n",
    "# Drop categorical features\n",
    "balanced_df_knn = balanced_df_knn.drop(columns=categorical_features)\n",
    "\n",
    "# Encode the target column\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(['phishing', 'legitimate'])  # Define a consistent order for labels\n",
    "balanced_df_knn['label_encoded'] = label_encoder.transform(balanced_df_knn['label'])\n",
    "\n",
    "# Drop the original label column\n",
    "balanced_df_knn = balanced_df_knn.drop(columns=['label'])\n",
    "\n",
    "# Scale numerical features\n",
    "numerical_columns = [col for col in balanced_df_knn.columns if col != 'label_encoded']\n",
    "scaler = StandardScaler()\n",
    "balanced_df_knn[numerical_columns] = scaler.fit_transform(balanced_df_knn[numerical_columns])\n",
    "\n",
    "# Train-test split\n",
    "X_knn = balanced_df_knn.drop(columns=['label_encoded'])  # Features\n",
    "y_knn = balanced_df_knn['label_encoded']  # Target\n",
    "\n",
    "X_train_knn, X_test_knn, y_train_knn, y_test_knn = train_test_split(\n",
    "    X_knn, y_knn, test_size=0.2, random_state=42, stratify=y_knn\n",
    ")\n",
    "\n",
    "# Save datasets\n",
    "X_train_knn.to_csv(os.path.join(folder, 'knn_X_train.csv'), index=False)\n",
    "X_test_knn.to_csv(os.path.join(folder, 'knn_X_test.csv'), index=False)\n",
    "y_train_knn.to_csv(os.path.join(folder, 'knn_y_train.csv'), index=False)\n",
    "y_test_knn.to_csv(os.path.join(folder, 'knn_y_test.csv'), index=False)\n",
    "\n",
    "# Summary\n",
    "print(f\"KNN datasets prepared and saved to: {folder}\")\n",
    "print(f\"X_train shape: {X_train_knn.shape}, y_train shape: {y_train_knn.shape}\")\n",
    "print(f\"X_test shape: {X_test_knn.shape}, y_test shape: {y_test_knn.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "1. **Handle Missing Values**:\n",
    "   - Rows with missing values should be removed or imputed to ensure a clean dataset.\n",
    "\n",
    "2. **Feature Scaling**:\n",
    "   - Standardize features to have a mean of 0 and standard deviation of 1. Logistic regression assumes features are on a comparable scale for optimal performance.\n",
    "\n",
    "3. **Label Encoding**:\n",
    "   - Convert target labels to numeric values (`0` and `1`).\n",
    "   - Ensure all categorical and Boolean features are also numeric.\n",
    "\n",
    "4. **Train-Test Split**:\n",
    "   - Divide the dataset into training and testing subsets to evaluate the model's performance on unseen data.\n",
    "\n",
    "5. **No Dimensionality Reduction**:\n",
    "   - Logistic regression does not strictly require techniques like PCA unless the dataset has a very high number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing completed! Files saved as lr_X_train.csv, lr_X_test.csv, lr_y_train.csv, and lr_y_test.csv.\n"
     ]
    }
   ],
   "source": [
    "# Define output folder for KNN\n",
    "folder = folder_name + '/logistic_regression/'\n",
    "ensure_folder_exists(folder)\n",
    "\n",
    "df_lr = balanced_df.copy()\n",
    "\n",
    "\n",
    "# Step 2: Drop irrelevant columns\n",
    "columns_to_drop = [\"url\", \"whois_data\", \"source\"]  # Adjust as necessary\n",
    "df_lr = df_lr.drop(columns=columns_to_drop, errors=\"ignore\")\n",
    "\n",
    "# Step 3: Handle missing values\n",
    "df_lr = df_lr.dropna()  # Drop rows with missing values\n",
    "\n",
    "# Step 4: Encode target labels\n",
    "label_encoder_lr = LabelEncoder()\n",
    "df_lr[\"label_encoded\"] = label_encoder_lr.fit_transform(df_lr[\"label\"])  # Binary encoding\n",
    "\n",
    "# Step 5: Convert Boolean columns to numeric\n",
    "boolean_columns_lr = df_lr.select_dtypes(include=[\"bool\"]).columns\n",
    "df_lr[boolean_columns_lr] = df_lr[boolean_columns_lr].astype(int)\n",
    "\n",
    "# Step 6: Separate features and target\n",
    "X_lr = df_lr.drop(columns=[\"label\", \"label_encoded\"])  # Features\n",
    "y_lr = df_lr[\"label_encoded\"]  # Target\n",
    "\n",
    "# Step 7: Feature scaling\n",
    "scaler_lr = StandardScaler()\n",
    "X_scaled_lr = scaler_lr.fit_transform(X_lr)\n",
    "\n",
    "# Step 8: Train-test split\n",
    "X_train_lr, X_test_lr, y_train_lr, y_test_lr = train_test_split(X_scaled_lr, y_lr, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 9: Save preprocessed data\n",
    "pd.DataFrame(X_train_lr).to_csv(os.path.join(folder, 'lr_X_train.csv'), index=False)\n",
    "pd.DataFrame(X_test_lr).to_csv(os.path.join(folder, 'lr_X_test.csv'), index=False)\n",
    "pd.DataFrame(y_train_lr).to_csv(os.path.join(folder, 'lr_y_train.csv'), index=False)\n",
    "pd.DataFrame(y_test_lr).to_csv(os.path.join(folder, 'lr_y_test.csv'), index=False)\n",
    "\n",
    "print(\"Preprocessing completed! Files saved as lr_X_train.csv, lr_X_test.csv, lr_y_train.csv, and lr_y_test.csv.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (f20dl)",
   "language": "python",
   "name": "f20dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
