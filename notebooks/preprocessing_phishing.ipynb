{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import zscore\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "folder_name = 'preprocessed_phishing'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1749311 entries, 0 to 2499998\n",
      "Data columns (total 18 columns):\n",
      " #   Column              Dtype  \n",
      "---  ------              -----  \n",
      " 0   url                 object \n",
      " 1   source              object \n",
      " 2   label               object \n",
      " 3   url_length          int64  \n",
      " 4   starts_with_ip      bool   \n",
      " 5   url_entropy         float64\n",
      " 6   has_punycode        bool   \n",
      " 7   digit_letter_ratio  float64\n",
      " 8   dot_count           int64  \n",
      " 9   at_count            int64  \n",
      " 10  dash_count          int64  \n",
      " 11  tld_count           int64  \n",
      " 12  domain_has_digits   bool   \n",
      " 13  subdomain_count     int64  \n",
      " 14  nan_char_entropy    float64\n",
      " 15  has_internal_links  bool   \n",
      " 16  whois_data          object \n",
      " 17  domain_age_days     float64\n",
      "dtypes: bool(4), float64(4), int64(6), object(4)\n",
      "memory usage: 206.9+ MB\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "data_path = '../data/raw_data/phishing/out.csv'  # adjust the path as necessary\n",
    "df = pd.read_csv(data_path)\n",
    "df.head()\n",
    "\n",
    "# drop null values\n",
    "df_clean = df.dropna()\n",
    "\n",
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "legitimate    1096403\n",
      "phishing       652908\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "phishing      50000\n",
      "legitimate    50000\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "phishing      5000\n",
      "legitimate    5000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_clean['label'].value_counts())\n",
    "\n",
    "# Using a subset\n",
    "target_size = 50000\n",
    "target_size1 = 5000 #for hierarchical clustering\n",
    "\n",
    "legitimate_sample = df_clean[df_clean['label'] == 'legitimate'].sample(n=target_size, random_state=42)\n",
    "phishing_sample = df_clean[df_clean['label'] == 'phishing'].sample(n=target_size, random_state=42)\n",
    "legitimate_sample1 = df_clean[df_clean['label'] == 'legitimate'].sample(n=target_size1, random_state=42)\n",
    "phishing_sample1 = df_clean[df_clean['label'] == 'phishing'].sample(n=target_size1, random_state=42)\n",
    "\n",
    "\n",
    "# Combining the samples\n",
    "balanced_df = pd.concat([legitimate_sample, phishing_sample])\n",
    "small_balanced_df = pd.concat([legitimate_sample1, phishing_sample1])\n",
    "\n",
    "# Shuffling\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "small_balanced_df = small_balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(balanced_df['label'].value_counts())\n",
    "print(small_balanced_df['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the balanced_df subset to a new CSV file\n",
    "file_path = os.path.join(folder_name, 'subset.csv')\n",
    "balanced_df.to_csv(file_path, index=False)\n",
    "\n",
    "file_path1 = os.path.join(folder_name, 'smaller_subset.csv')\n",
    "small_balanced_df.to_csv(file_path1, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Below we divide the data into test and train sets for supervisedd learning pre-processings**\n",
    "- Clustering does not require this because it is an unsupervised learning where labels arent required.\n",
    "\n",
    "**Scaling required for**\n",
    "- Clustering\n",
    "- Perceptron\n",
    "- K-Nearest Neighbors\t\n",
    "- Multi-Layer Perceptron\n",
    "\n",
    "**PCA required for**\n",
    "- Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set distribution:\n",
      "label\n",
      "legitimate    40000\n",
      "phishing      40000\n",
      "Name: count, dtype: int64\n",
      "Testing set distribution:\n",
      "label\n",
      "phishing      10000\n",
      "legitimate    10000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# %% Train-Test Split\n",
    "# Split the balanced dataset into train (80%) and test (20%)\n",
    "train_df, test_df = train_test_split(balanced_df, test_size=0.2, random_state=42, stratify=balanced_df['label'])\n",
    "\n",
    "# Save train and test datasets\n",
    "train_df.to_csv(os.path.join(folder_name, 'train.csv'), index=False)\n",
    "test_df.to_csv(os.path.join(folder_name, 'test.csv'), index=False)\n",
    "\n",
    "# Print class distributions\n",
    "print(f\"Training set distribution:\\n{train_df['label'].value_counts()}\")\n",
    "print(f\"Testing set distribution:\\n{test_df['label'].value_counts()}\")\n",
    "\n",
    "#for small\n",
    "train_df1, test_df1 = train_test_split(small_balanced_df, test_size=0.2, random_state=42, stratify=small_balanced_df['label'])\n",
    "train_df1.to_csv(os.path.join(folder_name, 'train_small.csv'), index=False)\n",
    "test_df1.to_csv(os.path.join(folder_name, 'test_small.csv'), index=False)\n",
    "# print(f\"Training set distribution:\\n{train_df1['label'].value_counts()}\")\n",
    "# print(f\"Testing set distribution:\\n{test_df1['label'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting relevant columns\n",
    "numerical_features = ['url_length', 'url_entropy', 'digit_letter_ratio',\n",
    "                     'dot_count', 'at_count', 'dash_count', 'tld_count',\n",
    "                     'subdomain_count', 'nan_char_entropy', 'domain_age_days']\n",
    "# Add boolean features to the list of numerical features\n",
    "boolean_features = ['starts_with_ip', 'has_punycode', 'domain_has_digits', 'has_internal_links']\n",
    "categorical_features = ['url', 'source', 'whois_data']\n",
    "\n",
    "# Combine all features\n",
    "all_features = numerical_features + boolean_features + categorical_features\n",
    "target_features = ['label']\n",
    "\n",
    "# Creating 3 dataframes (full data, train, test)\n",
    "# all_df = balanced_df[numerical_features + target_features]\n",
    "all_df = balanced_df[all_features + target_features].copy()\n",
    "train_df = train_df[all_features + target_features].copy()\n",
    "test_df = test_df[all_features + target_features].copy()\n",
    "\n",
    "# for small\n",
    "all_df1 = small_balanced_df[all_features + target_features].copy()\n",
    "train_df1 = train_df1[all_features + target_features].copy()\n",
    "test_df1 = test_df1[all_features + target_features].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Label Encoding for simplicity in this example\n",
    "for cat_feature in categorical_features:\n",
    "    label_encoder = LabelEncoder()\n",
    "    all_df[cat_feature] = label_encoder.fit_transform(all_df[cat_feature])\n",
    "    train_df[cat_feature] = label_encoder.transform(train_df[cat_feature])\n",
    "    test_df[cat_feature] = label_encoder.transform(test_df[cat_feature])\n",
    "\n",
    "    # For small subset\n",
    "    all_df1[cat_feature] = label_encoder.transform(all_df1[cat_feature])\n",
    "    train_df1[cat_feature] = label_encoder.transform(train_df1[cat_feature])\n",
    "    test_df1[cat_feature] = label_encoder.transform(test_df1[cat_feature])\n",
    "\n",
    "# Encode target labels\n",
    "all_df['label_encoded'] = label_encoder.fit_transform(all_df['label'])\n",
    "train_df['label_encoded'] = label_encoder.transform(train_df['label'])\n",
    "test_df['label_encoded'] = label_encoder.transform(test_df['label'])\n",
    "\n",
    "# For small subset\n",
    "all_df1['label_encoded'] = label_encoder.transform(all_df1['label'])\n",
    "train_df1['label_encoded'] = label_encoder.transform(train_df1['label'])\n",
    "test_df1['label_encoded'] = label_encoder.transform(test_df1['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "1. Remove highly correlated features.\n",
    "2. Remove outliers using z-scores.\n",
    "3. Scale the data.\n",
    "4. Perform PCA for dimensionality reduction.\n",
    "5. Visualize the PCA-transformed data.\n",
    "6. Apply and visualize clustering results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 14 columns):\n",
      " #   Column              Non-Null Count   Dtype  \n",
      "---  ------              --------------   -----  \n",
      " 0   url_length          100000 non-null  int64  \n",
      " 1   url_entropy         100000 non-null  float64\n",
      " 2   digit_letter_ratio  100000 non-null  float64\n",
      " 3   dot_count           100000 non-null  int64  \n",
      " 4   at_count            100000 non-null  int64  \n",
      " 5   dash_count          100000 non-null  int64  \n",
      " 6   tld_count           100000 non-null  int64  \n",
      " 7   subdomain_count     100000 non-null  int64  \n",
      " 8   nan_char_entropy    100000 non-null  float64\n",
      " 9   domain_age_days     100000 non-null  float64\n",
      " 10  starts_with_ip      100000 non-null  bool   \n",
      " 11  has_punycode        100000 non-null  bool   \n",
      " 12  domain_has_digits   100000 non-null  bool   \n",
      " 13  has_internal_links  100000 non-null  bool   \n",
      "dtypes: bool(4), float64(4), int64(6)\n",
      "memory usage: 8.0 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing for clustering\n",
    "clustering_df = all_df.drop(columns=['label', 'label_encoded'] + categorical_features)\n",
    "\n",
    "# Convert boolean features to numeric if needed\n",
    "# clustering_df = clustering_df.astype(float)\n",
    "\n",
    "print(clustering_df.info())\n",
    "\n",
    "#for small\n",
    "clustering_df1 = all_df1.drop(columns=['label', 'label_encoded'] + categorical_features)\n",
    "# print(clustering_df1.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove highly correlated features.\n",
    "correlation_matrix = clustering_df.corr()\n",
    "upper_tri = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column].abs() > 0.9)]\n",
    "clustering_df = clustering_df.drop(columns=to_drop)\n",
    "\n",
    "#for small\n",
    "correlation_matrix1 = clustering_df1.corr()\n",
    "upper_tri1 = correlation_matrix1.where(np.triu(np.ones(correlation_matrix1.shape), k=1).astype(bool))\n",
    "to_drop1 = [column for column in upper_tri1.columns if any(upper_tri1[column].abs() > 0.9)]\n",
    "clustering_df1 = clustering_df1.drop(columns=to_drop1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 84716 entries, 1 to 99999\n",
      "Data columns (total 14 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   url_length          84716 non-null  float64\n",
      " 1   url_entropy         84716 non-null  float64\n",
      " 2   digit_letter_ratio  84716 non-null  float64\n",
      " 3   dot_count           84716 non-null  float64\n",
      " 4   at_count            84716 non-null  float64\n",
      " 5   dash_count          84716 non-null  float64\n",
      " 6   tld_count           84716 non-null  float64\n",
      " 7   subdomain_count     84716 non-null  float64\n",
      " 8   nan_char_entropy    84716 non-null  float64\n",
      " 9   domain_age_days     84716 non-null  float64\n",
      " 10  starts_with_ip      84716 non-null  float64\n",
      " 11  has_punycode        84716 non-null  float64\n",
      " 12  domain_has_digits   84716 non-null  float64\n",
      " 13  has_internal_links  84716 non-null  float64\n",
      "dtypes: float64(14)\n",
      "memory usage: 9.7 MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 8479 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   url_length          8479 non-null   float64\n",
      " 1   url_entropy         8479 non-null   float64\n",
      " 2   digit_letter_ratio  8479 non-null   float64\n",
      " 3   dot_count           8479 non-null   float64\n",
      " 4   at_count            8479 non-null   float64\n",
      " 5   dash_count          8479 non-null   float64\n",
      " 6   tld_count           8479 non-null   float64\n",
      " 7   subdomain_count     8479 non-null   float64\n",
      " 8   nan_char_entropy    8479 non-null   float64\n",
      " 9   domain_age_days     8479 non-null   float64\n",
      " 10  starts_with_ip      8479 non-null   float64\n",
      " 11  has_punycode        8479 non-null   float64\n",
      " 12  domain_has_digits   8479 non-null   float64\n",
      " 13  has_internal_links  8479 non-null   float64\n",
      "dtypes: float64(14)\n",
      "memory usage: 993.6 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Remove outliers using z-scores.\n",
    "# Convert boolean features to numeric if needed\n",
    "clustering_df = clustering_df.astype(float)\n",
    "clustering_df = clustering_df[(np.abs(zscore(clustering_df)) < 3).all(axis=1)]\n",
    "print(clustering_df.info())\n",
    "\n",
    "#for small\n",
    "clustering_df1 = clustering_df1.astype(float)\n",
    "clustering_df1 = clustering_df1[(np.abs(zscore(clustering_df1)) < 3).all(axis=1)]\n",
    "print(clustering_df1.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "clustering_df_scaled = scaler.fit_transform(clustering_df)\n",
    "\n",
    "#for small\n",
    "clustering_df_scaled1 = scaler.fit_transform(clustering_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components to retain 90% variance: 5\n",
      "Explained variance with 5 components: [0.43197546 0.6129199  0.73473313 0.83369331 0.90888724]\n"
     ]
    }
   ],
   "source": [
    "# Perform PCA without specifying the number of components\n",
    "pca = PCA()\n",
    "pca_transformed = pca.fit_transform(clustering_df_scaled)\n",
    "\n",
    "# Calculate cumulative variance explained by each component\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Find the number of components that retain at least 90% variance\n",
    "n_components_90 = np.argmax(cumulative_variance >= 0.9) + 1\n",
    "print(f\"Number of components to retain 90% variance: {n_components_90}\")\n",
    "\n",
    "# Apply PCA with the required number of components\n",
    "pca = PCA(n_components=n_components_90)\n",
    "pca.fit_transform(clustering_df_scaled)\n",
    "\n",
    "# Verify the explained variance\n",
    "print(f\"Explained variance with {n_components_90} components: {np.cumsum(pca.explained_variance_ratio_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Components needed to display atleast 90% data would be 5, but we shall use 2d clustering, hence our PCA shall be with 2 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "clustering_ready_df = pca.fit_transform(clustering_df_scaled)\n",
    "\n",
    "#for small\n",
    "clustering_ready_df1 = pca.fit_transform(clustering_df_scaled1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after preprocessing: (84716, 2)\n",
      "Shape after preprocessing small: (8479, 2)\n"
     ]
    }
   ],
   "source": [
    "# Final clustering-ready DataFrame\n",
    "print(f\"Shape after preprocessing: {clustering_ready_df.shape}\")\n",
    "\n",
    "#for small\n",
    "print(f\"Shape after preprocessing small: {clustering_ready_df1.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = folder_name + '/clustering/'\n",
    "\n",
    "# Save the clustering-ready DataFrame with PCA-reduced data\n",
    "clustering_ready_df = pd.DataFrame(clustering_ready_df, columns=['PCA1', 'PCA2'])  # Ensure column names are present\n",
    "clustering_ready_df.to_csv(os.path.join(folder,'clustering.csv'), index=False)\n",
    "\n",
    "#for small\n",
    "clustering_ready_df1 = pd.DataFrame(clustering_ready_df1, columns=['PCA1', 'PCA2'])  # Ensure column names are present\n",
    "clustering_ready_df1.to_csv(os.path.join(folder,'clustering_small.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "\n",
    "1. Decision Trees are not sensitive to feature scaling or outliers.\n",
    "2. Categorical labels must be encoded.\n",
    "3. No need for dimensionality reduction (like PCA).\n",
    "4. Handles missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant features for supervised learning\n",
    "dt_features = all_features  # Already defined earlier\n",
    "\n",
    "# Training and testing data for Decision Trees\n",
    "X_train = train_df[dt_features].copy()\n",
    "y_train = train_df['label_encoded'].copy()\n",
    "\n",
    "X_test = test_df[dt_features].copy()\n",
    "y_test = test_df['label_encoded'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (80000, 17), Shape of X_test: (20000, 17)\n",
      "Sample of y_train:\n",
      "9567     0\n",
      "93297    0\n",
      "31947    0\n",
      "11663    1\n",
      "33026    0\n",
      "Name: label_encoded, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check processed data\n",
    "print(f\"Shape of X_train: {X_train.shape}, Shape of X_test: {X_test.shape}\")\n",
    "print(f\"Sample of y_train:\\n{y_train.head()}\")\n",
    "\n",
    "folder = folder_name + '/decision_tree/'\n",
    "\n",
    "# Save preprocessed Decision Tree data\n",
    "X_train.to_csv(os.path.join(folder, 'dt_X_train.csv'), index=False)\n",
    "X_test.to_csv(os.path.join(folder, 'dt_X_test.csv'), index=False)\n",
    "y_train.to_csv(os.path.join(folder, 'dt_y_train.csv'), index=False)\n",
    "y_test.to_csv(os.path.join(folder, 'dt_y_test.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "1. Handle missing values\n",
    "2. No strict need for feature scaling, but it can improve numerical stability\n",
    "3. No need for dimensionality reduction (like PCA).\n",
    "4. Ensure labels and all features are numerical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and testing data for Naive Bayes\n",
    "nb_features = numerical_features  # Already defined earlier\n",
    "X_train_nb = train_df[nb_features].copy()\n",
    "y_train_nb = train_df['label_encoded'].copy()\n",
    "\n",
    "X_test_nb = test_df[nb_features].copy()\n",
    "y_test_nb = test_df['label_encoded'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_nb_scaled: (80000, 10), Shape of X_test_nb_scaled: (20000, 10)\n",
      "Sample of y_train_nb:\n",
      "9567     0\n",
      "93297    0\n",
      "31947    0\n",
      "11663    1\n",
      "33026    0\n",
      "Name: label_encoded, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Scale features (optional, recommended for Gaussian Naive Bayes)\n",
    "X_train_nb_scaled = pd.DataFrame(scaler.fit_transform(X_train_nb), columns=nb_features)\n",
    "X_test_nb_scaled = pd.DataFrame(scaler.transform(X_test_nb), columns=nb_features)\n",
    "\n",
    "# Check processed data\n",
    "print(f\"Shape of X_train_nb_scaled: {X_train_nb_scaled.shape}, Shape of X_test_nb_scaled: {X_test_nb_scaled.shape}\")\n",
    "print(f\"Sample of y_train_nb:\\n{y_train_nb.head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessed Naive Bayes data\n",
    "folder = folder_name + '/naive_bayes/'\n",
    "\n",
    "X_train_nb_scaled.to_csv(os.path.join(folder, 'nb_X_train.csv'), index=False)\n",
    "X_test_nb_scaled.to_csv(os.path.join(folder, 'nb_X_test.csv'), index=False)\n",
    "y_train_nb.to_csv(os.path.join(folder, 'nb_y_train.csv'), index=False)\n",
    "y_test_nb.to_csv(os.path.join(folder, 'nb_y_test.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (f20dl)",
   "language": "python",
   "name": "f20dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
