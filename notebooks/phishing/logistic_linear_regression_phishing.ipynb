{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dba7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import classification_report, mean_squared_error, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Step 1: Load the preprocessed data\n",
    "X_train_lr = pd.read_csv(\"../../data/preprocessed_phishing/l_regression/lr_X_train.csv\")\n",
    "X_test_lr = pd.read_csv(\"../../data/preprocessed_phishing/l_regression/lr_X_test.csv\")\n",
    "y_train_lr = pd.read_csv(\"../../data/preprocessed_phishing/l_regression/lr_y_train.csv\").values.ravel()  # Ensure target is 1D\n",
    "y_test_lr = pd.read_csv(\"../../data/preprocessed_phishing/l_regression/lr_y_test.csv\").values.ravel()    # Ensure target is 1D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52520245",
   "metadata": {},
   "source": [
    "### Linear Regression Implementation\n",
    "\n",
    "\n",
    "**Model Initialization:**\n",
    "\n",
    "A linear regression model is initialized using the default configuration to model the relationship between the features and the target variable.\n",
    "Training the Model:\n",
    "\n",
    "The model is trained on the training dataset (lr_X_train, lr_y_train), estimating the coefficients for each feature.\n",
    "Model Evaluation:\n",
    "\n",
    "Predictions are made on both training and testing datasets.\n",
    "Evaluation metrics include:\n",
    "Mean Squared Error (MSE): Measures the average squared difference between predicted and actual values, computed for both training and testing sets.\n",
    "If PCA is applied, the regression results are visualized along the first principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a66a83e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Train MSE': 0.03058393252753603, 'Test MSE': 0.030813566363399224}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Linear Regression Model\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train_lr, y_train_lr)\n",
    "\n",
    "# Predictions and MSE for training and test sets\n",
    "y_train_pred_linear = lin_reg.predict(X_train_lr)\n",
    "y_test_pred_linear = lin_reg.predict(X_test_lr)\n",
    "mse_train_linear = mean_squared_error(y_train_lr, y_train_pred_linear)\n",
    "mse_test_linear = mean_squared_error(y_test_lr, y_test_pred_linear)\n",
    "\n",
    "\n",
    "# Results Summary\n",
    "linear_results = {\n",
    "    \"Train MSE\": mse_train_linear,\n",
    "    \"Test MSE\": mse_test_linear,\n",
    "}\n",
    "\n",
    "linear_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aed0d33",
   "metadata": {},
   "source": [
    "The dataset exhibits a linear relationship with moderate noise, suggesting it is well-suited for linear regression, generalizing well with unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6c266",
   "metadata": {
    "papermill": {
     "duration": 0.005863,
     "end_time": "2024-11-22T05:50:08.574300",
     "exception": false,
     "start_time": "2024-11-22T05:50:08.568437",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Logistic Regression Implementation\n",
    "\n",
    "In this block, we implement a **Logistic Regression** model using the preprocessed dataset. The steps include:\n",
    "\n",
    "1. **Loading Preprocessed Data**:\n",
    "   - The data is split into training (`lr_X_train`, `lr_y_train`) and testing (`lr_X_test`, `lr_y_test`) subsets.\n",
    "   - Logistic regression requires all features to be numerical and scaled, which has been ensured during preprocessing.\n",
    "\n",
    "2. **Model Initialization**:\n",
    "   - A logistic regression model is initialized with the following parameters:\n",
    "     - `max_iter=1000`: Allows the model sufficient iterations to converge for larger datasets.\n",
    "     - `random_state=42`: Ensures reproducibility of results.\n",
    "\n",
    "3. **Training the Model**:\n",
    "   - The model is trained on the training dataset, learning the relationship between features and the target variable.\n",
    "\n",
    "4. **Model Evaluation**:\n",
    "   - Predictions are made on the test set.\n",
    "   - Accuracy and a detailed classification report (precision, recall, F1-score) are generated to evaluate performance.\n",
    "\n",
    "5. **Cross-Validation**:\n",
    "   - A 10-fold cross-validation is performed to assess the model's generalization ability across different splits of the training dataset.\n",
    "   - Metrics include mean accuracy and standard deviation, indicating consistency across folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcabcd8",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-22T05:50:08.586607Z",
     "iopub.status.busy": "2024-11-22T05:50:08.586188Z",
     "iopub.status.idle": "2024-11-22T05:50:16.591296Z",
     "shell.execute_reply": "2024-11-22T05:50:16.588029Z"
    },
    "papermill": {
     "duration": 8.016081,
     "end_time": "2024-11-22T05:50:16.595717",
     "exception": false,
     "start_time": "2024-11-22T05:50:08.579636",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 1/1 [00:01<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Logistic Regression...\n",
      "\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Legitimate       1.00      1.00      1.00      9922\n",
      "    Phishing       1.00      1.00      1.00     10078\n",
      "\n",
      "    accuracy                           1.00     20000\n",
      "   macro avg       1.00      1.00      1.00     20000\n",
      "weighted avg       1.00      1.00      1.00     20000\n",
      "\n",
      "\n",
      "Performing 10-Fold Cross-Validation...\n",
      "\n",
      "Cross-Validation Accuracy Scores: [1.       1.       1.       1.       1.       1.       0.999875 1.\n",
      " 1.       1.      ]\n",
      "Mean Accuracy: 0.9999874999999999\n",
      "Standard Deviation: 3.750000000001252e-05\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "# Step 2: Initialize Logistic Regression\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Step 3: Train the model with a progress bar\n",
    "print(\"Training Logistic Regression...\")\n",
    "for _ in tqdm(range(1), desc=\"Training Progress\"):\n",
    "    log_reg.fit(X_train_lr, y_train_lr)\n",
    "\n",
    "# Step 4: Evaluate the model on the test set\n",
    "print(\"\\nEvaluating Logistic Regression...\")\n",
    "y_pred_lr = log_reg.predict(X_test_lr)\n",
    "\n",
    "# Step 5: Generate classification report\n",
    "accuracy_lr = accuracy_score(y_test_lr, y_pred_lr)\n",
    "report_lr = classification_report(y_test_lr, y_pred_lr, target_names=[\"Legitimate\", \"Phishing\"])\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nAccuracy: {accuracy_lr:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report_lr)\n",
    "\n",
    "# Step 6: Perform cross-validation\n",
    "print(\"\\nPerforming 10-Fold Cross-Validation...\")\n",
    "cv_scores_lr = cross_val_score(log_reg, X_train_lr, y_train_lr, cv=10, scoring='accuracy')\n",
    "\n",
    "# Display cross-validation results\n",
    "print(\"\\nCross-Validation Accuracy Scores:\", cv_scores_lr)\n",
    "print(\"Mean Accuracy:\", cv_scores_lr.mean())\n",
    "print(\"Standard Deviation:\", cv_scores_lr.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c75a7e",
   "metadata": {
    "papermill": {
     "duration": 0.009966,
     "end_time": "2024-11-22T05:50:16.615971",
     "exception": false,
     "start_time": "2024-11-22T05:50:16.606005",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Logistic Regression: Results and Analysis\n",
    "\n",
    "**Accuracy**:\n",
    "- The model achieved a perfect accuracy of **100%** on the test set, demonstrating excellent performance in classifying both legitimate and phishing cases.\n",
    "\n",
    "**Classification Report**:\n",
    "1. **Legitimate (Class 0)**:\n",
    "   - **Precision**: 1.00  \n",
    "     All predictions for legitimate cases were correct.\n",
    "   - **Recall**: 1.00  \n",
    "     The model identified all legitimate cases without error.\n",
    "   - **F1-Score**: 1.00  \n",
    "     Reflects a perfect balance between precision and recall.\n",
    "\n",
    "2. **Phishing (Class 1)**:\n",
    "   - **Precision**: 1.00  \n",
    "     All phishing predictions were correct.\n",
    "   - **Recall**: 1.00  \n",
    "     The model captured all phishing cases perfectly.\n",
    "   - **F1-Score**: 1.00  \n",
    "     Indicates flawless performance for phishing classification.\n",
    "\n",
    "3. **Macro and Weighted Averages**:\n",
    "   - Both averages are **1.00**, confirming balanced and exceptional classification across both classes.\n",
    "\n",
    "---\n",
    "\n",
    "**Cross-Validation Results**:\n",
    "1. **Accuracy Scores**:\n",
    "   - Individual fold scores are consistently **1.00**, with a single fold scoring **0.999875**.\n",
    "2. **Mean Accuracy**:\n",
    "   - The mean cross-validation accuracy is **99.9987%**, demonstrating remarkable generalization ability.\n",
    "3. **Standard Deviation**:\n",
    "   - The standard deviation of **0.0000375** reflects extremely low variance, ensuring consistent performance across all folds.\n",
    "\n",
    "---\n",
    "\n",
    "### Insights\n",
    "\n",
    "1. **Exceptional Performance**:\n",
    "   - The logistic regression model demonstrates perfect classification on both the test set and cross-validation, with negligible variance across folds.\n",
    "\n",
    "2. **Dataset Characteristics**:\n",
    "   - The high performance suggests that the features provide sufficient separation between classes, enabling the model to classify accurately.\n",
    "\n",
    "3. **Generalization**:\n",
    "   - The consistency in cross-validation scores highlights the model's ability to generalize well to unseen data.\n",
    "\n",
    "---\n",
    "### Next Steps: Ensuring Model Robustness\n",
    "\n",
    "To ensure the logistic regression model is not overfitting, we will:\n",
    "\n",
    "1. **Test the Model on Noisy Data**:\n",
    "   - Introduce Gaussian noise to the features in both training and testing datasets and evaluate the model's performance.\n",
    "   - This simulates real-world scenarios where data might contain inconsistencies or imperfections.\n",
    "  \n",
    "2.  **Adversarial Testing**:\n",
    "    - Introduce targeted perturbations to test the model's robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b6cff11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T05:50:16.639113Z",
     "iopub.status.busy": "2024-11-22T05:50:16.638582Z",
     "iopub.status.idle": "2024-11-22T05:50:17.140770Z",
     "shell.execute_reply": "2024-11-22T05:50:17.137407Z"
    },
    "papermill": {
     "duration": 0.518587,
     "end_time": "2024-11-22T05:50:17.144889",
     "exception": false,
     "start_time": "2024-11-22T05:50:16.626302",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Logistic Regression on noisy data...\n",
      "\n",
      "Evaluating Logistic Regression on noisy data...\n",
      "\n",
      "Accuracy on Noisy Data: 1.0000\n",
      "\n",
      "Classification Report on Noisy Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Legitimate       1.00      1.00      1.00      9922\n",
      "    Phishing       1.00      1.00      1.00     10078\n",
      "\n",
      "    accuracy                           1.00     20000\n",
      "   macro avg       1.00      1.00      1.00     20000\n",
      "weighted avg       1.00      1.00      1.00     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create noisy versions of the transformed datasets\n",
    "X_train_noisy_lr = X_train_lr + np.random.normal(0, 0.05, X_train_lr.shape)  # Add Gaussian noise to training data\n",
    "X_test_noisy_lr = X_test_lr + np.random.normal(0, 0.05, X_test_lr.shape)    # Add Gaussian noise to test data\n",
    "\n",
    "# Train the model on noisy training data\n",
    "print(\"\\nTraining Logistic Regression on noisy data...\")\n",
    "log_reg_noisy = LogisticRegression(max_iter=1000, random_state=42)\n",
    "log_reg_noisy.fit(X_train_noisy_lr, y_train_lr)\n",
    "\n",
    "# Evaluate the model on noisy test data\n",
    "print(\"\\nEvaluating Logistic Regression on noisy data...\")\n",
    "y_pred_noisy_lr = log_reg_noisy.predict(X_test_noisy_lr)\n",
    "\n",
    "# Calculate accuracy and classification report\n",
    "accuracy_noisy_lr = accuracy_score(y_test_lr, y_pred_noisy_lr)\n",
    "report_noisy_lr = classification_report(y_test_lr, y_pred_noisy_lr, target_names=[\"Legitimate\", \"Phishing\"])\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nAccuracy on Noisy Data: {accuracy_noisy_lr:.4f}\")\n",
    "print(\"\\nClassification Report on Noisy Data:\")\n",
    "print(report_noisy_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9086b140",
   "metadata": {
    "papermill": {
     "duration": 0.01037,
     "end_time": "2024-11-22T05:50:17.165879",
     "exception": false,
     "start_time": "2024-11-22T05:50:17.155509",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Logistic Regression: Results on Noisy Data\n",
    "\n",
    "**Accuracy**:\n",
    "- The model maintained perfect accuracy of **100%** on the noisy dataset, demonstrating exceptional robustness.\n",
    "\n",
    "**Classification Report**:\n",
    "1. **Legitimate (Class 0)**:\n",
    "   - **Precision**: 1.00  \n",
    "     All legitimate predictions were correct.\n",
    "   - **Recall**: 1.00  \n",
    "     The model identified all legitimate cases perfectly.\n",
    "   - **F1-Score**: 1.00  \n",
    "     Reflects flawless performance under noisy conditions.\n",
    "\n",
    "2. **Phishing (Class 1)**:\n",
    "   - **Precision**: 1.00  \n",
    "     All phishing predictions were correct.\n",
    "   - **Recall**: 1.00  \n",
    "     The model identified all phishing cases without error.\n",
    "   - **F1-Score**: 1.00  \n",
    "     Indicates balanced and perfect classification for phishing cases.\n",
    "\n",
    "3. **Macro and Weighted Averages**:\n",
    "   - Both averages remain at **1.00**, confirming robustness against noise.\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps: Adversarial Testing\n",
    "\n",
    "Adversarial testing is a method to evaluate a model's robustness by introducing **targeted perturbations** to the input data. These perturbations are designed to simulate situations where an adversary might intentionally manipulate data to deceive the model.\n",
    "\n",
    "In this test, we will:\n",
    "1. **Generate Adversarial Noise**:\n",
    "   - Small, random Gaussian noise will be added to the test dataset.\n",
    "   - The noise is scaled proportionally to the standard deviation of each feature to ensure meaningful perturbations.\n",
    "\n",
    "2. **Create an Adversarial Dataset**:\n",
    "   - The original test dataset will be combined with the generated noise to produce an adversarial version.\n",
    "\n",
    "3. **Evaluate the Model**:\n",
    "   - The trained logistic regression model will be evaluated on the adversarial dataset.\n",
    "   - Key metrics such as accuracy, precision, recall, and F1-score will be analyzed to assess the model's robustness against adversarial manipulation.\n",
    "\n",
    "This step will help determine if the model can maintain its performance even when faced with challenging, noisy inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddea0519",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T05:50:17.187192Z",
     "iopub.status.busy": "2024-11-22T05:50:17.186801Z",
     "iopub.status.idle": "2024-11-22T05:50:17.292665Z",
     "shell.execute_reply": "2024-11-22T05:50:17.288863Z"
    },
    "papermill": {
     "duration": 0.119783,
     "end_time": "2024-11-22T05:50:17.296328",
     "exception": false,
     "start_time": "2024-11-22T05:50:17.176545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Logistic Regression on adversarial data...\n",
      "\n",
      "Accuracy on Adversarial Data: 1.0000\n",
      "\n",
      "Classification Report on Adversarial Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Legitimate       1.00      1.00      1.00      9922\n",
      "    Phishing       1.00      1.00      1.00     10078\n",
      "\n",
      "    accuracy                           1.00     20000\n",
      "   macro avg       1.00      1.00      1.00     20000\n",
      "weighted avg       1.00      1.00      1.00     20000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Ensure the data is in NumPy array format for compatibility\n",
    "X_test_lr_np = X_test_lr.to_numpy()\n",
    "\n",
    "# Generate adversarial noise\n",
    "adversarial_noise = np.random.normal(0, 0.1, X_test_lr_np.shape) * X_test_lr_np.std(axis=0, keepdims=True) * 0.5\n",
    "\n",
    "# Create adversarial test dataset\n",
    "X_test_adversarial_lr = X_test_lr_np + adversarial_noise\n",
    "\n",
    "# Evaluate the model on adversarial test data\n",
    "print(\"\\nEvaluating Logistic Regression on adversarial data...\")\n",
    "y_pred_adversarial_lr = log_reg_noisy.predict(X_test_adversarial_lr)\n",
    "\n",
    "# Calculate accuracy and classification report\n",
    "accuracy_adversarial_lr = accuracy_score(y_test_lr, y_pred_adversarial_lr)\n",
    "report_adversarial_lr = classification_report(y_test_lr, y_pred_adversarial_lr, target_names=[\"Legitimate\", \"Phishing\"])\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nAccuracy on Adversarial Data: {accuracy_adversarial_lr:.4f}\")\n",
    "print(\"\\nClassification Report on Adversarial Data:\")\n",
    "print(report_adversarial_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9bea17",
   "metadata": {
    "papermill": {
     "duration": 0.010465,
     "end_time": "2024-11-22T05:50:17.317652",
     "exception": false,
     "start_time": "2024-11-22T05:50:17.307187",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Logistic Regression: Results on Adversarial Data\n",
    "\n",
    "**Accuracy**:\n",
    "- The model maintained perfect accuracy of **100%** on the adversarial dataset, demonstrating exceptional robustness against targeted perturbations.\n",
    "\n",
    "**Classification Report**:\n",
    "1. **Legitimate (Class 0)**:\n",
    "   - **Precision**: 1.00  \n",
    "     All legitimate predictions were correct.\n",
    "   - **Recall**: 1.00  \n",
    "     The model identified all legitimate cases perfectly.\n",
    "   - **F1-Score**: 1.00  \n",
    "     Flawless classification for legitimate cases.\n",
    "\n",
    "2. **Phishing (Class 1)**:\n",
    "   - **Precision**: 1.00  \n",
    "     All phishing predictions were correct.\n",
    "   - **Recall**: 1.00  \n",
    "     The model captured all phishing cases without error.\n",
    "   - **F1-Score**: 1.00  \n",
    "     Perfect classification for phishing cases.\n",
    "\n",
    "3. **Macro and Weighted Averages**:\n",
    "   - Both averages remain at **1.00**, confirming balanced performance across both classes, even with adversarial inputs.\n",
    "\n",
    "---\n",
    "\n",
    "### Insights\n",
    "\n",
    "1. **Robustness to Adversarial Inputs**:\n",
    "   - The model successfully handled adversarial perturbations without any degradation in performance, highlighting its resilience.\n",
    "\n",
    "2. **Balanced Classification**:\n",
    "   - The model retained perfect precision, recall, and F1-scores across both classes, even under challenging conditions.\n",
    "\n",
    "3. **Final Validation**:\n",
    "   - These results confirm that the logistic regression model is robust and well-calibrated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489b6eb9",
   "metadata": {
    "papermill": {
     "duration": 0.005814,
     "end_time": "2024-11-22T05:50:17.334903",
     "exception": false,
     "start_time": "2024-11-22T05:50:17.329089",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Additional Tests for Logistic Regression\n",
    "\n",
    "To further validate and refine the logistic regression model, we will perform the following tests:\n",
    "\n",
    "1. **Feature Importance and Analysis**:\n",
    "   - **Purpose**: Identify which features contribute the most to the model's predictions.\n",
    "   - **Approach**: Extract model coefficients and rank features by their impact.\n",
    "   - **Benefit**: Helps ensure the model is leveraging meaningful features and not overfitting to irrelevant data.\n",
    "\n",
    "2. **Correlated Noise Testing**:\n",
    "   - **Purpose**: Evaluate the model's robustness to structured, correlated noise.\n",
    "   - **Approach**: Add systematic perturbations to specific columns (e.g., `url_length` or `dot_count`) and test the model's performance.\n",
    "   - **Benefit**: Confirms resilience to systematic changes or adversarial attempts based on correlated feature manipulation.\n",
    "\n",
    "3. **Class Imbalance Testing**:\n",
    "   - **Purpose**: Simulate imbalanced scenarios to test the model's ability to handle minority classes effectively.\n",
    "   - **Approach**: Modify the test set to have an extreme imbalance (e.g., 90% legitimate, 10% phishing) and evaluate the model.\n",
    "   - **Benefit**: Ensures the model can classify minority classes without bias.\n",
    "\n",
    "4. **Feature Removal Testing**:\n",
    "   - **Purpose**: Evaluate how the model performs if specific features are removed or corrupted.\n",
    "   - **Approach**: Sequentially drop individual features (or groups of features) and measure the impact on accuracy.\n",
    "   - **Benefit**: Ensures that the model does not overly rely on specific features and can adapt if some data becomes unavailable.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "We will begin with **Feature Importance and Analysis** and proceed sequentially."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4331238",
   "metadata": {
    "papermill": {
     "duration": 0.005673,
     "end_time": "2024-11-22T05:50:17.346436",
     "exception": false,
     "start_time": "2024-11-22T05:50:17.340763",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Feature Importance Analysis\n",
    "\n",
    "Feature importance analysis helps identify which features contribute the most to the logistic regression model's predictions. Understanding these contributions allows for better interpretability and potential refinement of the model.\n",
    "\n",
    "---\n",
    "\n",
    "**What We Are Doing**:\n",
    "1. **Extract Model Coefficients**:\n",
    "   - The logistic regression model assigns a coefficient to each feature, indicating its contribution to the prediction.\n",
    "\n",
    "2. **Rank Features by Importance**:\n",
    "   - Features are ranked based on the absolute value of their coefficients. Larger coefficients (positive or negative) indicate a greater impact on the model's decisions.\n",
    "\n",
    "3. **Display Top Features**:\n",
    "   - The top 10 features are displayed to highlight the most influential attributes driving the model's performance.\n",
    "\n",
    "---\n",
    "\n",
    "### Code for Feature Importance and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6153ef9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T05:50:17.360879Z",
     "iopub.status.busy": "2024-11-22T05:50:17.360301Z",
     "iopub.status.idle": "2024-11-22T05:50:17.380876Z",
     "shell.execute_reply": "2024-11-22T05:50:17.379122Z"
    },
    "papermill": {
     "duration": 0.031362,
     "end_time": "2024-11-22T05:50:17.383704",
     "exception": false,
     "start_time": "2024-11-22T05:50:17.352342",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance (Top 10 Features):\n",
      "   Feature  Coefficient  Abs_Coefficient\n",
      "11      11     8.943489         8.943489\n",
      "0        0     8.188846         8.188846\n",
      "2        2     3.614554         3.614554\n",
      "10      10    -0.951891         0.951891\n",
      "5        5    -0.940916         0.940916\n",
      "13      13    -0.646416         0.646416\n",
      "7        7     0.221642         0.221642\n",
      "12      12     0.081941         0.081941\n",
      "9        9    -0.035299         0.035299\n",
      "8        8     0.031990         0.031990\n"
     ]
    }
   ],
   "source": [
    "# Assuming the logistic regression model (log_reg) is already trained\n",
    "# Extract feature names from the training dataset\n",
    "feature_names = X_train_lr.columns\n",
    "\n",
    "# Extract model coefficients\n",
    "coefficients = log_reg.coef_[0]  # Coefficients for the logistic regression model\n",
    "\n",
    "# Combine feature names and coefficients into a DataFrame\n",
    "feature_importance = pd.DataFrame({\n",
    "    \"Feature\": feature_names,\n",
    "    \"Coefficient\": coefficients\n",
    "})\n",
    "\n",
    "# Add an absolute coefficient column to sort by magnitude\n",
    "feature_importance[\"Abs_Coefficient\"] = np.abs(feature_importance[\"Coefficient\"])\n",
    "\n",
    "# Sort features by absolute coefficient\n",
    "feature_importance = feature_importance.sort_values(by=\"Abs_Coefficient\", ascending=False)\n",
    "\n",
    "# Display top features\n",
    "print(\"Feature Importance (Top 10 Features):\")\n",
    "print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec842ad",
   "metadata": {
    "papermill": {
     "duration": 0.00578,
     "end_time": "2024-11-22T05:50:17.395954",
     "exception": false,
     "start_time": "2024-11-22T05:50:17.390174",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Analysis of Feature Importance\n",
    "\n",
    "**RN I CANNOT GET THE ACTUAL FEATURE NAME BECAUSE FOR THAT WE NEED THE SUBSET.CSV WHICH SUCKS SINCE WE DO NOT DO PREPROCESSING IN THE SAME FILE SO GETTING INFO LIKE THIS IS HARD SINCE WE NEED THE ORIGINAL DATASET. NP IGNORE AND WE CAN MOVE ON**\n",
    "\n",
    "**Top Features**:\n",
    "The top 10 features ranked by their absolute coefficients provide insight into the most influential attributes in the logistic regression model:\n",
    "- **Feature 11**: Largest positive impact with a coefficient of **8.94**.\n",
    "- **Feature 0**: Second most important, also with a strong positive effect (**8.19**).\n",
    "- **Feature 2**: Significant positive contribution (**3.61**).\n",
    "- **Feature 10** and **Feature 5**: Most influential negative contributors, with coefficients of **-0.95** and **-0.94**, respectively.\n",
    "\n",
    "**Insights**:\n",
    "\n",
    "1. **Dominance of Features**:\n",
    "   - Features **11**, **0**, and **2** have a significantly higher impact compared to others.\n",
    "   - These features are likely driving the model's high performance.\n",
    "\n",
    "2. **Lesser Impact Features**:\n",
    "   - Features like **8** and **9** have negligible coefficients, indicating minimal contribution.\n",
    "---\n",
    "\n",
    "Let's move on with **Correlated Noise Testing**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01729b9",
   "metadata": {
    "papermill": {
     "duration": 0.005859,
     "end_time": "2024-11-22T05:50:17.408372",
     "exception": false,
     "start_time": "2024-11-22T05:50:17.402513",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Correlated Noise Testing\n",
    "\n",
    "Correlated noise testing evaluates the model's robustness by introducing structured noise into specific, high-impact features. This test simulates real-world scenarios where certain features might be systematically altered or perturbed.\n",
    "\n",
    "---\n",
    "\n",
    "**What We Are Doing**:\n",
    "1. **Select High-Impact Features**:\n",
    "   - Based on feature importance analysis, the top three influential features (`Feature 11`, `Feature 0`, and `Feature 2`) are selected for testing.\n",
    "\n",
    "2. **Generate Correlated Noise**:\n",
    "   - Gaussian noise is added to the selected features, scaled based on each feature's standard deviation to ensure realistic perturbations.\n",
    "\n",
    "3. **Evaluate Model Performance**:\n",
    "   - The trained logistic regression model is tested on the noisy dataset.\n",
    "   - Accuracy and classification metrics (precision, recall, F1-score) are analyzed to assess the model's robustness against structured noise.\n",
    "\n",
    "---\n",
    "###  Code for Correlated Noise Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a03f29c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T05:50:17.422199Z",
     "iopub.status.busy": "2024-11-22T05:50:17.421662Z",
     "iopub.status.idle": "2024-11-22T05:50:17.515552Z",
     "shell.execute_reply": "2024-11-22T05:50:17.511955Z"
    },
    "papermill": {
     "duration": 0.104655,
     "end_time": "2024-11-22T05:50:17.518944",
     "exception": false,
     "start_time": "2024-11-22T05:50:17.414289",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Logistic Regression on test data with correlated noise...\n",
      "\n",
      "Accuracy on Correlated Noisy Data: 1.0000\n",
      "\n",
      "Classification Report on Correlated Noisy Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Legitimate       1.00      1.00      1.00      9922\n",
      "    Phishing       1.00      1.00      1.00     10078\n",
      "\n",
      "    accuracy                           1.00     20000\n",
      "   macro avg       1.00      1.00      1.00     20000\n",
      "weighted avg       1.00      1.00      1.00     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select high-impact features from the feature importance ranking\n",
    "high_impact_features = [11, 0, 2]  # Replace with actual feature names if needed\n",
    "\n",
    "# Add correlated noise to the high-impact features in the test dataset\n",
    "X_test_correlated_lr = X_test_lr.copy()  # Copy to avoid modifying the original test set\n",
    "\n",
    "# Generate noise correlated with the original features\n",
    "for feature in high_impact_features:\n",
    "    noise = np.random.normal(0, 0.05, X_test_correlated_lr.shape[0]) * X_test_lr.iloc[:, feature].std() * 0.5\n",
    "    X_test_correlated_lr.iloc[:, feature] += noise\n",
    "\n",
    "# Evaluate the model on the noisy test data\n",
    "print(\"\\nEvaluating Logistic Regression on test data with correlated noise...\")\n",
    "y_pred_correlated_lr = log_reg.predict(X_test_correlated_lr)\n",
    "\n",
    "# Calculate accuracy and classification report\n",
    "accuracy_correlated_lr = accuracy_score(y_test_lr, y_pred_correlated_lr)\n",
    "report_correlated_lr = classification_report(y_test_lr, y_pred_correlated_lr, target_names=[\"Legitimate\", \"Phishing\"])\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nAccuracy on Correlated Noisy Data: {accuracy_correlated_lr:.4f}\")\n",
    "print(\"\\nClassification Report on Correlated Noisy Data:\")\n",
    "print(report_correlated_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464cdc88",
   "metadata": {
    "papermill": {
     "duration": 0.010646,
     "end_time": "2024-11-22T05:50:17.540806",
     "exception": false,
     "start_time": "2024-11-22T05:50:17.530160",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Logistic Regression: Results on Correlated Noisy Data\n",
    "\n",
    "**Accuracy**:\n",
    "- The model maintained perfect accuracy of **100%** on the test set with correlated noise, demonstrating exceptional robustness.\n",
    "\n",
    "**Classification Report**:\n",
    "1. **Legitimate (Class 0)**:\n",
    "   - **Precision**: 1.00  \n",
    "     All legitimate predictions were correct.\n",
    "   - **Recall**: 1.00  \n",
    "     The model identified all legitimate cases perfectly.\n",
    "   - **F1-Score**: 1.00  \n",
    "     Flawless classification for legitimate cases.\n",
    "\n",
    "2. **Phishing (Class 1)**:\n",
    "   - **Precision**: 1.00  \n",
    "     All phishing predictions were correct.\n",
    "   - **Recall**: 1.00  \n",
    "     The model identified all phishing cases without error.\n",
    "   - **F1-Score**: 1.00  \n",
    "     Perfect classification for phishing cases.\n",
    "\n",
    "3. **Macro and Weighted Averages**:\n",
    "   - Both averages remain at **1.00**, confirming the model's resilience to structured, correlated noise.\n",
    "\n",
    "---\n",
    "\n",
    "**Insights**:\n",
    "- The model's performance was unaffected by the introduction of structured noise to high-impact features.\n",
    "- This result highlights the robustness of the logistic regression model under challenging conditions.\n",
    "\n",
    "---\n",
    "\n",
    "Let’s move on with **Class Imbalance Testing**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37663ce0",
   "metadata": {
    "papermill": {
     "duration": 0.00828,
     "end_time": "2024-11-22T05:50:17.560103",
     "exception": false,
     "start_time": "2024-11-22T05:50:17.551823",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Class Imbalance Testing\n",
    "\n",
    "Class imbalance is a common challenge in real-world datasets where one class significantly outweighs the other. This test simulates an imbalanced scenario in the test data to evaluate the model's ability to classify minority classes effectively.\n",
    "\n",
    "---\n",
    "\n",
    "**What We Are Doing**:\n",
    "1. **Create an Imbalanced Test Set**:\n",
    "   - Modify the test dataset to have an imbalance, with 80% of samples belonging to the `Legitimate` class and 20% to the `Phishing` class.\n",
    "\n",
    "2. **Evaluate Model Performance**:\n",
    "   - Test the trained logistic regression model on this imbalanced dataset.\n",
    "   - Analyze accuracy and detailed classification metrics (precision, recall, F1-score) to assess if the model can still classify minority samples accurately.\n",
    "\n",
    "---\n",
    "\n",
    "### Code for Class Imbalance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8e33091",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T05:50:17.574262Z",
     "iopub.status.busy": "2024-11-22T05:50:17.573833Z",
     "iopub.status.idle": "2024-11-22T05:50:17.635134Z",
     "shell.execute_reply": "2024-11-22T05:50:17.634061Z"
    },
    "papermill": {
     "duration": 0.072392,
     "end_time": "2024-11-22T05:50:17.638728",
     "exception": false,
     "start_time": "2024-11-22T05:50:17.566336",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Logistic Regression on imbalanced test data...\n",
      "\n",
      "Accuracy on Imbalanced Data: 1.0000\n",
      "\n",
      "Classification Report on Imbalanced Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Legitimate       1.00      1.00      1.00      9922\n",
      "    Phishing       1.00      1.00      1.00      4000\n",
      "\n",
      "    accuracy                           1.00     13922\n",
      "   macro avg       1.00      1.00      1.00     13922\n",
      "weighted avg       1.00      1.00      1.00     13922\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert y_test_lr to a Pandas Series for indexing\n",
    "y_test_lr_series = pd.Series(y_test_lr)\n",
    "\n",
    "# Create a highly imbalanced test dataset (e.g., 80% Legitimate, 20% Phishing)\n",
    "legitimate_indices = y_test_lr_series[y_test_lr_series == 0].index\n",
    "phishing_indices = y_test_lr_series[y_test_lr_series == 1].index\n",
    "\n",
    "# Calculate the maximum number of samples that can be used for imbalance\n",
    "legitimate_sample_size = int(0.8 * len(y_test_lr_series))\n",
    "phishing_sample_size = len(y_test_lr_series) - legitimate_sample_size\n",
    "\n",
    "# Downsample Legitimate or Phishing cases to achieve imbalance\n",
    "imbalanced_legitimate = resample(\n",
    "    legitimate_indices, replace=False, n_samples=min(legitimate_sample_size, len(legitimate_indices)), random_state=42\n",
    ")\n",
    "imbalanced_phishing = resample(\n",
    "    phishing_indices, replace=False, n_samples=min(phishing_sample_size, len(phishing_indices)), random_state=42\n",
    ")\n",
    "\n",
    "# Combine indices and create the imbalanced test set\n",
    "imbalanced_indices = np.hstack((imbalanced_legitimate, imbalanced_phishing))\n",
    "X_test_imbalanced_lr = X_test_lr.iloc[imbalanced_indices]\n",
    "y_test_imbalanced_lr = y_test_lr_series.iloc[imbalanced_indices]\n",
    "\n",
    "# Evaluate the model on the imbalanced test data\n",
    "print(\"\\nEvaluating Logistic Regression on imbalanced test data...\")\n",
    "y_pred_imbalanced_lr = log_reg.predict(X_test_imbalanced_lr)\n",
    "\n",
    "# Calculate accuracy and classification report\n",
    "accuracy_imbalanced_lr = accuracy_score(y_test_imbalanced_lr, y_pred_imbalanced_lr)\n",
    "report_imbalanced_lr = classification_report(\n",
    "    y_test_imbalanced_lr, y_pred_imbalanced_lr, target_names=[\"Legitimate\", \"Phishing\"]\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nAccuracy on Imbalanced Data: {accuracy_imbalanced_lr:.4f}\")\n",
    "print(\"\\nClassification Report on Imbalanced Data:\")\n",
    "print(report_imbalanced_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13378ca0",
   "metadata": {
    "papermill": {
     "duration": 0.010719,
     "end_time": "2024-11-22T05:50:17.660883",
     "exception": false,
     "start_time": "2024-11-22T05:50:17.650164",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Logistic Regression: Results on Imbalanced Test Data\n",
    "\n",
    "**Accuracy**:\n",
    "- The model maintained perfect accuracy of **100%** on the imbalanced test dataset, showcasing its robustness.\n",
    "\n",
    "**Classification Report**:\n",
    "1. **Legitimate (Class 0)**:\n",
    "   - **Precision**: 1.00  \n",
    "     All predictions for legitimate cases were correct.\n",
    "   - **Recall**: 1.00  \n",
    "     The model successfully identified all legitimate cases in the imbalanced dataset.\n",
    "   - **F1-Score**: 1.00  \n",
    "     Flawless classification for the majority class.\n",
    "\n",
    "2. **Phishing (Class 1)**:\n",
    "   - **Precision**: 1.00  \n",
    "     All phishing predictions were correct.\n",
    "   - **Recall**: 1.00  \n",
    "     The model accurately captured all phishing cases despite their minority representation.\n",
    "   - **F1-Score**: 1.00  \n",
    "     Perfect classification for the minority class.\n",
    "\n",
    "3. **Macro and Weighted Averages**:\n",
    "   - Both averages remain at **1.00**, confirming that the model performs equally well across both majority and minority classes.\n",
    "\n",
    "---\n",
    "\n",
    "### Insights\n",
    "\n",
    "1. **Robustness to Class Imbalance**:\n",
    "   - The model exhibited no performance degradation even under heavily imbalanced conditions, highlighting its ability to classify minority classes effectively.\n",
    "\n",
    "2. **Balanced Predictions**:\n",
    "   - The equal precision, recall, and F1-scores for both classes indicate that the model is not biased toward the majority class.\n",
    "\n",
    "---\n",
    "\n",
    "Let’s move on with **Feature Removal Testing** to explore potential performance optimizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112d1c82",
   "metadata": {
    "papermill": {
     "duration": 0.011022,
     "end_time": "2024-11-22T05:50:17.683009",
     "exception": false,
     "start_time": "2024-11-22T05:50:17.671987",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Feature Removal Testing\n",
    "\n",
    "This test evaluates how the logistic regression model performs when specific features are removed. It helps identify:\n",
    "- Features that the model heavily relies on.\n",
    "- Redundant features that have minimal impact on accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "**What We Are Doing**:\n",
    "1. **Sequential Feature Removal**:\n",
    "   - Drop one feature at a time from the training and test datasets.\n",
    "   - Retrain the logistic regression model without the dropped feature.\n",
    "\n",
    "2. **Evaluate Model Performance**:\n",
    "   - Measure the accuracy of the reduced model on the test dataset.\n",
    "   - Compare the reduced accuracy with the original model’s accuracy to determine the impact of each feature.\n",
    "\n",
    "3. **Analyze Impact**:\n",
    "   - Features that cause significant accuracy drops are critical for the model’s performance.\n",
    "   - Features with minimal impact may be candidates for removal to simplify the model.\n",
    "\n",
    "---\n",
    "### Code for Robustness to Feature Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba88bcb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T05:50:17.709017Z",
     "iopub.status.busy": "2024-11-22T05:50:17.707447Z",
     "iopub.status.idle": "2024-11-22T05:50:23.593850Z",
     "shell.execute_reply": "2024-11-22T05:50:23.592786Z"
    },
    "papermill": {
     "duration": 5.9033,
     "end_time": "2024-11-22T05:50:23.597494",
     "exception": false,
     "start_time": "2024-11-22T05:50:17.694194",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Robustness to Feature Removal...\n",
      "Feature '0' removed. Accuracy: 0.9988\n",
      "Feature '1' removed. Accuracy: 1.0000\n",
      "Feature '2' removed. Accuracy: 1.0000\n",
      "Feature '3' removed. Accuracy: 1.0000\n",
      "Feature '4' removed. Accuracy: 1.0000\n",
      "Feature '5' removed. Accuracy: 1.0000\n",
      "Feature '6' removed. Accuracy: 1.0000\n",
      "Feature '7' removed. Accuracy: 1.0000\n",
      "Feature '8' removed. Accuracy: 1.0000\n",
      "Feature '9' removed. Accuracy: 1.0000\n",
      "Feature '10' removed. Accuracy: 1.0000\n",
      "Feature '11' removed. Accuracy: 0.9392\n",
      "Feature '12' removed. Accuracy: 1.0000\n",
      "Feature '13' removed. Accuracy: 1.0000\n",
      "\n",
      "Features with the Highest Impact on Accuracy:\n",
      "   Feature  Accuracy  Accuracy Drop\n",
      "11      11   0.93920        0.06080\n",
      "0        0   0.99875        0.00125\n",
      "13      13   0.99995        0.00005\n",
      "1        1   1.00000        0.00000\n",
      "2        2   1.00000        0.00000\n",
      "3        3   1.00000        0.00000\n",
      "4        4   1.00000        0.00000\n",
      "5        5   1.00000        0.00000\n",
      "6        6   1.00000        0.00000\n",
      "7        7   1.00000        0.00000\n"
     ]
    }
   ],
   "source": [
    "# Store the original model's accuracy for comparison\n",
    "original_accuracy = log_reg.score(X_test_lr, y_test_lr)\n",
    "\n",
    "# Evaluate the model by sequentially dropping each feature\n",
    "print(\"\\nEvaluating Robustness to Feature Removal...\")\n",
    "feature_impact = []\n",
    "\n",
    "for feature in X_train_lr.columns:\n",
    "    # Drop the current feature\n",
    "    X_train_dropped = X_train_lr.drop(columns=[feature])\n",
    "    X_test_dropped = X_test_lr.drop(columns=[feature])\n",
    "    \n",
    "    # Retrain the model on the reduced dataset\n",
    "    log_reg_dropped = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    log_reg_dropped.fit(X_train_dropped, y_train_lr)\n",
    "    \n",
    "    # Evaluate the model on the reduced test set\n",
    "    dropped_accuracy = log_reg_dropped.score(X_test_dropped, y_test_lr)\n",
    "    \n",
    "    # Store the results\n",
    "    feature_impact.append((feature, dropped_accuracy))\n",
    "    print(f\"Feature '{feature}' removed. Accuracy: {dropped_accuracy:.4f}\")\n",
    "\n",
    "# Convert results to a DataFrame for analysis\n",
    "feature_impact_df = pd.DataFrame(feature_impact, columns=[\"Feature\", \"Accuracy\"])\n",
    "feature_impact_df[\"Accuracy Drop\"] = original_accuracy - feature_impact_df[\"Accuracy\"]\n",
    "\n",
    "# Display features with the highest impact\n",
    "print(\"\\nFeatures with the Highest Impact on Accuracy:\")\n",
    "print(feature_impact_df.sort_values(by=\"Accuracy Drop\", ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8469c6ee",
   "metadata": {
    "papermill": {
     "duration": 0.012574,
     "end_time": "2024-11-22T05:50:23.625162",
     "exception": false,
     "start_time": "2024-11-22T05:50:23.612588",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Logistic Regression: Results of Feature Removal Test\n",
    "\n",
    "**Evaluation Results**:\n",
    "- The model was evaluated by sequentially removing each feature and measuring the impact on accuracy.\n",
    "- Key findings from the test are:\n",
    "\n",
    "1. **Features with the Highest Impact**:\n",
    "   - **Feature '11'**: Caused the most significant accuracy drop (from **1.0000** to **0.9392**) when removed.\n",
    "   - **Feature '0'**: Resulted in a minor accuracy drop (from **1.0000** to **0.9988**).\n",
    "\n",
    "2. **Redundant Features**:\n",
    "   - Several features (e.g., **1**, **2**, **3**, etc.) caused no measurable drop in accuracy when removed, indicating their minimal impact on the model’s performance.\n",
    "\n",
    "---\n",
    "\n",
    "**Insights**:\n",
    "1. **Critical Features**:\n",
    "   - **Feature '11'** is crucial for the model's performance and should be retained in all scenarios.\n",
    "   - **Feature '0'** is also somewhat impactful and contributes to the model's overall robustness.\n",
    "\n",
    "2. **Potential Redundancy**:\n",
    "   - Features that caused no accuracy drop (e.g., **'1'**, **'2'**, etc.) might be redundant and could potentially be removed to simplify the model without sacrificing performance.\n",
    "\n",
    "---\n",
    "\n",
    "The results demonstrate that the model is robust and does not overly depend on most features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b030ba5e",
   "metadata": {
    "papermill": {
     "duration": 0.012569,
     "end_time": "2024-11-22T05:50:23.675842",
     "exception": false,
     "start_time": "2024-11-22T05:50:23.663273",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Hyperparameter Tuning for Logistic Regression\n",
    "\n",
    "Hyperparameter tuning aims to optimize the logistic regression model by systematically searching for the best combination of parameters. This process ensures that the model achieves peak performance on the given dataset.\n",
    "\n",
    "---\n",
    "\n",
    "**What We Are Doing**:\n",
    "1. **Define Hyperparameter Grid**:\n",
    "   - Tune key parameters:\n",
    "     - **`C`**: Controls the strength of regularization.\n",
    "     - **`solver`**: Specifies the optimization algorithm.\n",
    "     - **`penalty`**: Determines the type of regularization (`l1` or `l2`).\n",
    "\n",
    "2. **Grid Search with Cross-Validation**:\n",
    "   - Use 10-fold cross-validation to evaluate multiple combinations of parameters.\n",
    "   - Identify the best configuration based on accuracy.\n",
    "\n",
    "3. **Evaluate Best Model**:\n",
    "   - Test the best hyperparameter configuration on the test dataset.\n",
    "   - Generate accuracy and a detailed classification report.\n",
    "\n",
    "---\n",
    "### Code for Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebfffb50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T05:50:23.702258Z",
     "iopub.status.busy": "2024-11-22T05:50:23.701866Z",
     "iopub.status.idle": "2024-11-22T13:08:22.725133Z",
     "shell.execute_reply": "2024-11-22T13:08:22.721839Z"
    },
    "papermill": {
     "duration": 26279.063826,
     "end_time": "2024-11-22T13:08:22.752750",
     "exception": false,
     "start_time": "2024-11-22T05:50:23.688924",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing Grid Search for Hyperparameter Tuning...\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "50 fits failed out of a total of 300.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "50 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.9996375       nan 0.9997375 0.9995875 0.9995875 0.9995875 0.9999625\n",
      "       nan 0.9998875 0.9998125 0.999825  0.9998    0.9999875       nan\n",
      " 0.9998625 0.9999875 0.9999875 0.9998625 0.9999625       nan 0.9998625\n",
      " 0.9999875 0.9999875 0.9998625 0.999975        nan 0.9998625 0.9999875\n",
      " 0.999975  0.9998625]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating the Best Logistic Regression Model...\n",
      "\n",
      "Best Hyperparameters: {'C': 1, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "\n",
      "Accuracy of Best Model: 1.0000\n",
      "\n",
      "Classification Report of Best Model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Legitimate       1.00      1.00      1.00      9922\n",
      "    Phishing       1.00      1.00      1.00     10078\n",
      "\n",
      "    accuracy                           1.00     20000\n",
      "   macro avg       1.00      1.00      1.00     20000\n",
      "weighted avg       1.00      1.00      1.00     20000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    \"C\": [0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
    "    \"solver\": [\"liblinear\", \"lbfgs\", \"saga\"],  # Optimization algorithms\n",
    "    \"penalty\": [\"l1\", \"l2\"]  # Regularization techniques\n",
    "}\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "log_reg_tune = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=log_reg_tune,\n",
    "    param_grid=param_grid,\n",
    "    cv=10,  # 10-fold cross-validation\n",
    "    scoring=\"accuracy\",\n",
    "    verbose=1,  # Display progress\n",
    "    n_jobs=-1  # Use all available CPU cores\n",
    ")\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "print(\"\\nPerforming Grid Search for Hyperparameter Tuning...\")\n",
    "grid_search.fit(X_train_lr, y_train_lr)\n",
    "\n",
    "# Get the best parameters and model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "print(\"\\nEvaluating the Best Logistic Regression Model...\")\n",
    "y_pred_best_lr = best_model.predict(X_test_lr)\n",
    "\n",
    "# Calculate accuracy and classification report\n",
    "accuracy_best_lr = accuracy_score(y_test_lr, y_pred_best_lr)\n",
    "report_best_lr = classification_report(y_test_lr, y_pred_best_lr, target_names=[\"Legitimate\", \"Phishing\"])\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nBest Hyperparameters: {best_params}\")\n",
    "print(f\"\\nAccuracy of Best Model: {accuracy_best_lr:.4f}\")\n",
    "print(\"\\nClassification Report of Best Model:\")\n",
    "print(report_best_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7271ea59",
   "metadata": {
    "papermill": {
     "duration": 0.012397,
     "end_time": "2024-11-22T13:08:22.783000",
     "exception": false,
     "start_time": "2024-11-22T13:08:22.770603",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Logistic Regression: Hyperparameter Tuning Results\n",
    "\n",
    "The hyperparameter tuning process identified the best configuration for the logistic regression model, further optimizing its performance.\n",
    "\n",
    "---\n",
    "\n",
    "**Best Parameters**:\n",
    "- **C**: `1`  \n",
    "   Controls the strength of regularization, with `1` indicating moderate regularization.\n",
    "- **Penalty**: `l1`  \n",
    "   Applies lasso regularization, which can help with feature selection by shrinking less important coefficients to zero.\n",
    "- **Solver**: `liblinear`  \n",
    "   A solver optimized for small datasets and supports `l1` regularization.\n",
    "\n",
    "---\n",
    "\n",
    "**Model Performance**:\n",
    "1. **Accuracy**:\n",
    "   - Achieved perfect accuracy of **1.0000** on the test dataset.\n",
    "\n",
    "2. **Classification Report**:\n",
    "   - **Legitimate (Class 0)**:\n",
    "     - **Precision**: 1.00  \n",
    "     - **Recall**: 1.00  \n",
    "     - **F1-Score**: 1.00  \n",
    "   - **Phishing (Class 1)**:\n",
    "     - **Precision**: 1.00  \n",
    "     - **Recall**: 1.00  \n",
    "     - **F1-Score**: 1.00  \n",
    "   - Macro and Weighted Averages: All metrics remain at **1.00**, confirming the model's reliability.\n",
    "\n",
    "---\n",
    "\n",
    "**Observations**:\n",
    "1. **Robustness**:\n",
    "   - The model continues to demonstrate exceptional performance even after hyperparameter tuning.\n",
    "\n",
    "2. **Feature Selection**:\n",
    "   - The use of `l1` regularization likely reduced reliance on less important features, enhancing efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "**Warnings During Tuning**:\n",
    "1. **Convergence Issues**:\n",
    "   - Some solvers reached the maximum number of iterations without converging.\n",
    "   - This did not affect the final results, as `liblinear` produced the optimal configuration.\n",
    "\n",
    "2. **Unsupported Parameter Combinations**:\n",
    "   - The solver `lbfgs` does not support `l1` penalty, resulting in failed fits for certain combinations.\n",
    "   - These fits were skipped, and valid combinations were evaluated.\n",
    "\n",
    "---\n",
    "\n",
    "This tuned model highlights the efficiency of logistic regression when regularization and optimization are carefully selected."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6139644,
     "sourceId": 9978227,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python (f20dl)",
   "language": "python",
   "name": "f20dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 26300.422131,
   "end_time": "2024-11-22T13:08:25.422105",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-22T05:50:04.999974",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
