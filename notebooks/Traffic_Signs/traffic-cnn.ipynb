{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10038402,"sourceType":"datasetVersion","datasetId":6183440},{"sourceId":10038419,"sourceType":"datasetVersion","datasetId":6183452}],"dockerImageVersionId":30787,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports and Constants\nThis cell imports the necessary libraries and sets up constants for paths, image size, batch size, number of epochs, and K-Fold splits. \n- Libraries include **TensorFlow/Keras** for deep learning, OpenCV for image processing, and utilities for metrics and plotting.\n- Paths specify the directories for training, testing data, and labels.\n","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport cv2\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport json\nimport itertools\n\n# Paths\nTRAIN_DIR = '/kaggle/input/preprocessed-data/traffic/trainNew'\nTEST_DIR = '/kaggle/input/preprocessed-data/traffic/testNew'\nLABELS_PATH = '/kaggle/input/labels-csv/labels.csv'\n\n# Parameters\nIMAGE_SIZE = (100, 100)  # Resize images\nBATCH_SIZE = 32\nEPOCHS = 100\nKFOLDS = 10\n\nprint(\"Imports and constants loaded.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load Labels and Helper Functions\n- Loads the label mapping from a CSV file, mapping each class ID to its corresponding name.\n- Defines a helper function `load_data` to load images from the training or test directory. The function:\n  - Reads and resizes images to a standard size.\n  - Normalizes the image data.\n  - Returns NumPy arrays of image data and corresponding labels.\n","metadata":{}},{"cell_type":"code","source":"# Load labels\nlabels_df = pd.read_csv(LABELS_PATH)\nclass_mapping = dict(zip(labels_df['ClassId'], labels_df['Name']))\n\n# Function to load data\ndef load_data(data_dir, image_size, is_test=False):\n    data, labels = [], []\n    if is_test:\n        for img_name in os.listdir(data_dir):\n            img_path = os.path.join(data_dir, img_name)\n            try:\n                label = int(img_name.split(\"_\")[1])  # Adjust based on filename structure\n            except ValueError:\n                continue\n            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n            if img is not None:\n                img = cv2.resize(img, image_size)\n                data.append(img)\n                labels.append(label)\n    else:\n        for class_name in os.listdir(data_dir):\n            class_dir = os.path.join(data_dir, class_name)\n            label = int(class_name)\n            for img_name in os.listdir(class_dir):\n                img_path = os.path.join(class_dir, img_name)\n                img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n                if img is not None:\n                    img = cv2.resize(img, image_size)\n                    data.append(img)\n                    labels.append(label)\n    data = np.array(data).reshape(-1, image_size[0], image_size[1], 1) / 255.0\n    labels = np.array(labels)\n    return data, labels\n\nprint(\"Helper functions loaded.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-28T14:46:41.676902Z","iopub.execute_input":"2024-11-28T14:46:41.677175Z","iopub.status.idle":"2024-11-28T14:46:55.236879Z","shell.execute_reply.started":"2024-11-28T14:46:41.677146Z","shell.execute_reply":"2024-11-28T14:46:55.235986Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load Data\n- Loads the training and testing datasets using the `load_data` function.\n- One-hot encodes the training and testing labels for use in categorical classification.\n- Splits the training data into training and validation sets (80/20 split).\n- Prints the shapes of the training, validation, and test datasets, along with the number of classes.\n","metadata":{}},{"cell_type":"code","source":"print(\"Loading training data...\")\nX_train, Y_train = load_data(TRAIN_DIR, IMAGE_SIZE)\nNUM_CLASSES = np.max(Y_train) + 1\nY_train_one_hot = to_categorical(Y_train, NUM_CLASSES)\n\nprint(\"Loading test data...\")\nX_test, Y_test = load_data(TEST_DIR, IMAGE_SIZE, is_test=True)\nY_test_one_hot = to_categorical(Y_test, NUM_CLASSES)\n\n# Split training data for validation\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train_one_hot, test_size=0.2, random_state=42)\n\nprint(f\"Training data: {X_train.shape}, Test data: {X_test.shape}, Classes: {NUM_CLASSES}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CNN Model Creator Function\nDefines the function `create_cnn_model` to create a customizable CNN architecture. It allows for:\n- Adjustable number of convolutional layers and filters.\n- Configurable dense layers, units, kernel size, and dropout rate.\n- Compilation using the Adam optimizer with categorical cross-entropy loss and accuracy as a metric.\n","metadata":{}},{"cell_type":"code","source":"# Function to create a CNN model dynamically\ndef create_cnn_model(input_shape, num_classes, conv_layers, filters, kernel_size, dense_layers, dense_units, dropout_rate):\n    model = Sequential()\n    for i in range(conv_layers):\n        if i == 0:\n            model.add(Conv2D(filters, kernel_size, activation='relu', input_shape=input_shape))\n        else:\n            model.add(Conv2D(filters * (2 ** i), kernel_size, activation='relu'))\n        model.add(MaxPooling2D((2, 2)))\n        model.add(Dropout(dropout_rate))\n    model.add(Flatten())\n    for _ in range(dense_layers):\n        model.add(Dense(dense_units, activation='relu'))\n        model.add(Dropout(dropout_rate))\n    model.add(Dense(num_classes, activation='softmax'))\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\nprint(\"Model generator ready.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Hyperparameter Grid\n- Defines a grid of hyperparameters for tuning the CNN model.\n- Uses `itertools.product` to generate all possible combinations of parameters (e.g., layers, filters, dropout rate).\n- Saves the combinations to a JSON file for subsequent testing.\n","metadata":{}},{"cell_type":"code","source":"# Hyperparameter grid\nparam_grid = {\n    \"conv_layers\": [4],\n    \"filters\": [32, 64],\n    \"kernel_size\": [(3, 3)],\n    \"dense_layers\": [1, 2],\n    \"dense_units\": [512],\n    \"dropout_rate\": [0.5]\n}\n\n# Generate all combinations and save to a file\nparam_combinations = list(itertools.product(\n    param_grid[\"conv_layers\"],\n    param_grid[\"filters\"],\n    param_grid[\"kernel_size\"],\n    param_grid[\"dense_layers\"],\n    param_grid[\"dense_units\"],\n    param_grid[\"dropout_rate\"]\n))\nwith open(\"/kaggle/working/param_combinations.json\", \"w\") as f:\n    json.dump(param_combinations, f)\n\nprint(f\"{len(param_combinations)} parameter combinations saved.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluate a Parameter Combination with K-Fold Cross-Validation\nDefines the `evaluate_combination` function, which:\n- Performs K-Fold Cross-Validation to split data into training and validation folds.\n- Trains the model on `n-1` folds and evaluates it on the remaining fold.\n- Tracks metrics (accuracy, precision, recall, F1 score) for each fold.\n- Returns the average metrics across folds.\n","metadata":{}},{"cell_type":"code","source":"# Function to evaluate a parameter combination using K-Fold Cross-Validation\ndef evaluate_combination(params, X, Y, input_shape, num_classes):\n    conv_layers, filters, kernel_size, dense_layers, dense_units, dropout_rate = params\n    kfold = KFold(n_splits=KFOLDS, shuffle=True, random_state=42)\n    fold_metrics = []\n\n    for fold, (train_idx, val_idx) in enumerate(kfold.split(X)):\n        print(f\"Fold {fold + 1}/{KFOLDS}\")\n        X_fold_train, X_fold_val = X[train_idx], X[val_idx]\n        Y_fold_train, Y_fold_val = Y[train_idx], Y[val_idx]\n\n        model = create_cnn_model(input_shape, num_classes, conv_layers, filters, kernel_size, dense_layers, dense_units, dropout_rate)\n        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n\n        model.fit(X_fold_train, Y_fold_train, validation_data=(X_fold_val, Y_fold_val), epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=0, callbacks=[early_stopping])\n\n        Y_val_pred = model.predict(X_fold_val)\n        Y_val_pred_classes = np.argmax(Y_val_pred, axis=1)\n        Y_val_true_classes = np.argmax(Y_fold_val, axis=1)\n\n        metrics = {\n            \"accuracy\": accuracy_score(Y_val_true_classes, Y_val_pred_classes),\n            \"precision\": precision_score(Y_val_true_classes, Y_val_pred_classes, average='weighted'),\n            \"recall\": recall_score(Y_val_true_classes, Y_val_pred_classes, average='weighted'),\n            \"f1_score\": f1_score(Y_val_true_classes, Y_val_pred_classes, average='weighted'),\n        }\n        fold_metrics.append(metrics)\n        print(f\"Metrics for Fold {fold + 1}: {metrics}\")\n\n    avg_metrics = {k: np.mean([m[k] for m in fold_metrics]) for k in fold_metrics[0]}\n    return avg_metrics\n\nprint(\"Evaluation function ready.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load Parameter Combinations and Evaluate\n- Loads hyperparameter combinations from the JSON file.\n- Evaluates the first five parameter combinations using the `evaluate_combination` function.\n- Saves the results and metrics for each combination to separate JSON files.\n","metadata":{}},{"cell_type":"code","source":"# Load parameter combinations\nwith open(\"/kaggle/working/param_combinations.json\", \"r\") as f:\n    param_combinations = json.load(f)\n\nresults = []\nfor idx, params in enumerate(param_combinations[:5]):  # Limit to first 5 for faster testing\n    print(f\"Evaluating combination {idx + 1}/{len(param_combinations)}: {params}\")\n    metrics = evaluate_combination(params, X_train, Y_train, X_train[0].shape, NUM_CLASSES)\n    results.append({\"params\": params, \"metrics\": metrics})\n    with open(f\"/kaggle/working/results_{idx + 1}.json\", \"w\") as f:\n        json.dump({\"params\": params, \"metrics\": metrics}, f)\n\nprint(\"Grid search complete.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Test the Best Model\n- Identifies the best parameter combination based on F1 score from the results.\n- Trains the model with the best parameters on the full training set.\n- Evaluates the model on the test set and prints the test metrics.\n","metadata":{}},{"cell_type":"code","source":"# Test the best model\nbest_params = max(results, key=lambda x: x[\"metrics\"][\"f1_score\"])[\"params\"]\nprint(f\"Best parameters: {best_params}\")\n\nmodel = create_cnn_model(X_train[0].shape, NUM_CLASSES, *best_params)\nmodel.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=1)\ntest_metrics = model.evaluate(X_test, Y_test_one_hot, verbose=1)\nprint(f\"Test metrics: {test_metrics}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Save the Best Model\n- Trains the model using the best parameter combination on the full dataset.\n- Saves the trained model to a file for future use.\n","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\n\n# Save the best model\nbest_params = max(results, key=lambda x: x[\"metrics\"][\"f1_score\"])[\"params\"]\nprint(f\"Best parameters: {best_params}\")\n\n# Train the model with best parameters on the full training data\nbest_model = create_cnn_model(X_train[0].shape, NUM_CLASSES, *best_params)\nbest_model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=1)\n\n# Save the trained model\nmodel_save_path = \"/kaggle/working/best_model.h5\"\nbest_model.save(model_save_path)\nprint(f\"Best model saved to {model_save_path}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluate Model on Validation Data\nDefines the `evaluate_model_on_validation` function to:\n- Predict class labels for validation or test data.\n- Compute accuracy, precision, recall, and F1 score.\n- Return the metrics for evaluation.\n","metadata":{}},{"cell_type":"code","source":"# Function to evaluate a model on validation or test data\ndef evaluate_model_on_validation(model, X_val, Y_val):\n    Y_val_pred = model.predict(X_val)\n    Y_val_pred_classes = np.argmax(Y_val_pred, axis=1)\n    Y_val_true_classes = np.argmax(Y_val, axis=1)\n\n    metrics = {\n        \"accuracy\": accuracy_score(Y_val_true_classes, Y_val_pred_classes),\n        \"precision\": precision_score(Y_val_true_classes, Y_val_pred_classes, average='weighted'),\n        \"recall\": recall_score(Y_val_true_classes, Y_val_pred_classes, average='weighted'),\n        \"f1_score\": f1_score(Y_val_true_classes, Y_val_pred_classes, average='weighted'),\n    }\n    return metrics\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Compare Saved and Current Models\n- Loads the previously saved model and evaluates it on the validation set.\n- Compares its F1 score with the current model's score.\n- Replaces the saved model if the current model performs better.\n","metadata":{}},{"cell_type":"code","source":"import os\nfrom tensorflow.keras.models import load_model\n\n# Filepath for the saved model\nmodel_save_path = \"/kaggle/working/best_model.h5\"\n\n# Check if the last saved model exists\nif os.path.exists(model_save_path):\n    print(\"Loading last saved model for comparison...\")\n    last_saved_model = load_model(model_save_path)\n    last_metrics = evaluate_model_on_validation(last_saved_model, X_val, Y_val)\n    print(f\"Last saved model metrics: {last_metrics}\")\nelse:\n    print(\"No previously saved model found.\")\n    last_metrics = None\n\n# Train the current best model\ncurrent_model = create_cnn_model(X_train[0].shape, NUM_CLASSES, *best_params)\ncurrent_model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=1)\n\n# Evaluate the current model\ncurrent_metrics = evaluate_model_on_validation(current_model, X_val, Y_val)\nprint(f\"Current model metrics: {current_metrics}\")\n\n# Compare F1 scores and replace if the current model is better\nif last_metrics is None or current_metrics[\"f1_score\"] > last_metrics[\"f1_score\"]:\n    print(\"Current model is better. Replacing the last saved model.\")\n    current_model.save(model_save_path)\nelse:\n    print(\"Last saved model is better. Keeping the previous model.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Plot Training History\nDefines a function `plot_training_history` to visualize:\n- Training vs. validation accuracy over epochs.\n- Training vs. validation loss over epochs.\nThis helps analyze model convergence and diagnose overfitting or underfitting.\n","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Assuming history is the training history object from the final fold\n\ndef plot_training_history(history):\n    \"\"\"Plots training and validation accuracy and loss over epochs.\"\"\"\n    # Extract metrics\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    epochs = range(1, len(acc) + 1)\n\n    # Plot accuracy\n    plt.figure(figsize=(12, 5))\n\n    # Training vs Validation Accuracy\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, acc, label='Training Accuracy')\n    plt.plot(epochs, val_acc, label='Validation Accuracy')\n    plt.title('Training and Validation Accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n\n    # Training vs Validation Loss\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, loss, label='Training Loss')\n    plt.plot(epochs, val_loss, label='Validation Loss')\n    plt.title('Training and Validation Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n\n    plt.tight_layout()\n    plt.show()\n\n\nplot_training_history(final_fold_history)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}